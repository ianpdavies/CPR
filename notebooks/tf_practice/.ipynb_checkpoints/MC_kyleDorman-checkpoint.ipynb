{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ggplot import *\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "import pandas as pd\n",
    "import math\n",
    "from tensorflow.contrib import distributions\n",
    "from scipy.misc import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard categorical cross entropy\n",
    "# N data points, C classes\n",
    "# true - true values. Shape: (N, C)\n",
    "# pred - predicted values. Shape: (N, C)\n",
    "# returns - loss (N)\n",
    "def categorical_cross_entropy(true, pred):\n",
    "    return np.sum(true * np.log(pred), axis=1)\n",
    "\n",
    "# Bayesian categorical cross entropy.\n",
    "# N data points, C classes, T monte carlo simulations\n",
    "# true - true values. Shape: (N, C)\n",
    "# pred_var - predicted logit values and variance. Shape: (N, C + 1)\n",
    "# returns - loss (N,)\n",
    "def bayesian_categorical_crossentropy(T, num_classes):\n",
    "    def bayesian_categorical_crossentropy_internal(true, pred_var):\n",
    "        # shape: (N,)\n",
    "        std = K.sqrt(pred_var[:, num_classes:])\n",
    "        # shape: (N,)\n",
    "        variance = pred_var[:, num_classes]\n",
    "        variance_depressor = K.exp(variance) - K.ones_like(variance)\n",
    "        # shape: (N, C)\n",
    "        pred = pred_var[:, 0:num_classes]\n",
    "        # shape: (N,)\n",
    "        undistorted_loss = K.categorical_crossentropy(pred, true, from_logits=True)\n",
    "        # shape: (T,)\n",
    "        iterable = K.variable(np.ones(T))\n",
    "        dist = distributions.Normal(loc=K.zeros_like(std), scale=std)\n",
    "        monte_carlo_results = K.map_fn(gaussian_categorical_crossentropy(true, pred, dist, undistorted_loss, num_classes), iterable, name='monte_carlo_results')\n",
    "\n",
    "        variance_loss = K.mean(monte_carlo_results, axis=0) * undistorted_loss\n",
    "\n",
    "        return variance_loss + undistorted_loss + variance_depressor\n",
    "  \n",
    "    return bayesian_categorical_crossentropy_internal\n",
    "\n",
    "# for a single monte carlo simulation, \n",
    "#   calculate categorical_crossentropy of \n",
    "#   predicted logit values plus gaussian \n",
    "#   noise vs true values.\n",
    "# true - true values. Shape: (N, C)\n",
    "# pred - predicted logit values. Shape: (N, C)\n",
    "# dist - normal distribution to sample from. Shape: (N, C)\n",
    "# undistorted_loss - the crossentropy loss without variance distortion. Shape: (N,)\n",
    "# num_classes - the number of classes. C\n",
    "# returns - total differences for all classes (N,)\n",
    "def gaussian_categorical_crossentropy(true, pred, dist, undistorted_loss, num_classes):\n",
    "    def map_fn(i):\n",
    "        std_samples = K.transpose(dist.sample(num_classes))\n",
    "        distorted_loss = K.categorical_crossentropy(pred + std_samples, true, from_logits=True)\n",
    "        diff = undistorted_loss - distorted_loss\n",
    "        return -K.elu(diff)\n",
    "    return map_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet50(input_shape):\n",
    "    input_tensor = tf.keras.layers.Input(shape=input_shape)\n",
    "    base_model = tf.keras.applications.ResNet50(include_top=False, input_tensor=input_tensor)\n",
    "    # freeze encoder layers to prevent over fitting\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    output_tensor = tf.keras.layers.Flatten()(base_model.output)\n",
    "    return tf.keras.Model(inputs=input_tensor, outputs=output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x132324c9dd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50((224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bayesian_model(encoder, input_shape, output_classes):\n",
    "    encoder_model = tf.keras.applications.ResNet50(input_shape)\n",
    "    input_tensor = tf.keras.layers.Input(shape=encoder_model.output_shape[1:])\n",
    "    x = tf.keras.BatchNormalization(name='post_encoder')(input_tensor)\n",
    "    x = tf.keras.Dropout(0.5)(x)\n",
    "    x = tf.keras.Dense(500, activation='relu')(x)\n",
    "    x = tf.keras.BatchNormalization()(x)\n",
    "    x = tf.keras.Dropout(0.5)(x)\n",
    "    x = tf.keras.Dense(100, activation='relu')(x)\n",
    "    x = tf.keras.BatchNormalization()(x)\n",
    "    x = tf.keras.Dropout(0.5)(x)\n",
    "\n",
    "    logits = tf.keras.Dense(output_classes)(x)\n",
    "    variance_pre = tf.keras.Dense(1)(x)\n",
    "    variance = tf.keras.Activation('softplus', name='variance')(variance_pre)\n",
    "    logits_variance = tf.keras.concatenate([logits, variance], name='logits_variance')\n",
    "    softmax_output = tf.keras.Activation('softmax', name='softmax_output')(logits)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_tensor, outputs=[logits_variance,softmax_output])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bc2d6c603a8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m model.compile(\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     loss={'logits_variance': bayesian_categorical_crossentropy(100, 10),\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(lr=1e-3, decay=0.001),\n",
    "    loss={'logits_variance': bayesian_categorical_crossentropy(100, 10),\n",
    "          'softmax_output': 'categorical_crossentropy'},\n",
    "    metrics={'softmax_output': metrics.categorical_accuracy},\n",
    "    loss_weights={'logits_variance': .2, 'softmax_output': 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_min_input_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-925d8d372812>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# flags.DEFINE_integer('patience', 20, 'Early stopping epochs patience to wait before stopping.')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmin_image_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_min_input_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_train_batch_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder_min_input_size' is not defined"
     ]
    }
   ],
   "source": [
    "# flags = tf.app.flags\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "# flags.DEFINE_string('dataset', 'cifar10', 'The dataset to train the model on.')\n",
    "# flags.DEFINE_string('encoder', 'resnet50', 'The encoder model to train from.')\n",
    "# flags.DEFINE_integer('epochs', 1, 'Number of training examples.')\n",
    "# flags.DEFINE_integer('monte_carlo_simulations', 100, 'The number of monte carlo simulations to run for the aleatoric categorical crossentroy loss function.')\n",
    "# flags.DEFINE_integer('batch_size', 32, 'The batch size for the generator')\n",
    "# flags.DEFINE_boolean('debug', False, 'If this is for debugging the model/training process or not.')\n",
    "# flags.DEFINE_integer('verbose', 0, 'Whether to use verbose logging when constructing the data object.')\n",
    "# flags.DEFINE_boolean('stop', True, 'Stop aws instance after finished running.')\n",
    "# flags.DEFINE_float('min_delta', 0.005, 'Early stopping minimum change value.')\n",
    "# flags.DEFINE_integer('patience', 20, 'Early stopping epochs patience to wait before stopping.')\n",
    "\n",
    "min_image_size = encoder_min_input_size(FLAGS.encoder)\n",
    "((x_train, y_train), (x_test, y_test)) = test_train_batch_data(FLAGS.dataset, FLAGS.encoder, FLAGS.debug, augment_data=True)\n",
    "\n",
    "min_image_size = list(min_image_size)\n",
    "min_image_size.append(3)\n",
    "num_classes = y_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = create_bayesian_model(FLAGS.encoder, min_image_size, num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
