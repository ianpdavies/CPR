{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN on multiple images under varying cloud cover %s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ee\n",
    "from IPython import display\n",
    "# import math\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "# from osgeo import gdal\n",
    "# import tempfile\n",
    "import tensorflow as tf\n",
    "# import urllib\n",
    "import rasterio\n",
    "from zipfile import *\n",
    "import math\n",
    "from math import sqrt\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to stacking tifs, preprocessing, splitting data into train/val, and training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tifStacker(path, img, feat_list_new, overwrite=False): \n",
    "\n",
    "    \"\"\" Reorders the tifs (i.e. individual bands) downloaded from GEE according to feature order in feat_list_new, \n",
    "    then stacks them all into one multiband image called 'stack.tif' located in input path. Requires rasterio, \n",
    "    os, from zipfile import *\n",
    "    \n",
    "    Ideally want to have this function in another notebook and call it, but running into problems - ZipFile not found \n",
    "    \n",
    "    from ipynb.fs.full.useful_funcs import tifStacker\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str \n",
    "        Path to image folder\n",
    "    img :str \n",
    "        Name of image file (without file extension)\n",
    "    feat_list_new : list\n",
    "        List of feature names (str) to be the desired order of the output stacked .tif - target feature must be last\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    \"stacked.tif\" in 'path' location \n",
    "    feat_list_files : list \n",
    "        Not sure what that is or what it's for \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    file_list = []\n",
    "    path = path+'images/'+img\n",
    "    \n",
    "    # This gets the name of all files in the zip folder, and formats them into a full path readable by rasterio.open()\n",
    "    with ZipFile(path + '/' + img + '.zip', 'r') as f:\n",
    "        names = f.namelist()\n",
    "        names = ['zip://'+ path + '/' + img + '.zip!' +name for name in names]\n",
    "        for file in names:\n",
    "            if file.endswith('.tif'):\n",
    "                file_list.append(file)\n",
    "    \n",
    "    feat_list_files = list(map(lambda x: x.split('.')[-2], file_list)) # Grabs a list of features in file order        \n",
    "    \n",
    "    if overwrite=True:\n",
    "        # Remove stack file if already exists\n",
    "        try:\n",
    "            os.remove(path + '/stack/stack.tif')\n",
    "            print('Removing existing \"stack.tif\" and creating new one')\n",
    "        except FileNotFoundError:\n",
    "            print('No existing \"stack.tif\". Creating \"stack.tif\"')\n",
    "            \n",
    "        # Create 1 row df of file names where each col is a feature name, in the order files are stored locally\n",
    "        file_arr = pd.DataFrame(data=[file_list], columns=feat_list_files)\n",
    "\n",
    "        # Then index the file list by the ordered list of feature names used in training\n",
    "        file_arr = file_arr.loc[:, feat_list_new]\n",
    "\n",
    "        # The take this re-ordered row as a list - the new file_list\n",
    "        file_list = list(file_arr.iloc[0,:])\n",
    "\n",
    "        # Read metadata of first file. This needs to be a band in float32 dtype, because it sets the metadata for the entire stack\n",
    "        # and we are converting the other bands to float64\n",
    "        with rasterio.open(file_list[1]) as src0:\n",
    "            meta = src0.meta\n",
    "            meta['dtype'] = 'float32'\n",
    "        #         print(meta)\n",
    "\n",
    "        # Update meta to reflect the number of layers\n",
    "        meta.update(count = len(file_list))\n",
    "\n",
    "        # Read each layer, convert to float, and write it to stack\n",
    "        # There's also a gdal way to do this, but unsure how to convert to float: https://gis.stackexchange.com/questions/223910/using-rasterio-or-gdal-to-stack-multiple-bands-without-using-subprocess-commands\n",
    "\n",
    "        # Make new directory for stacked tif if it doesn't already exist\n",
    "        try:\n",
    "            os.mkdir(path +'/stack')\n",
    "        except FileExistsError:\n",
    "            print('Stack directory already exists') \n",
    "            \n",
    "        with rasterio.open(path + '/stack/stack.tif', 'w', **meta) as dst:\n",
    "            for id, layer in enumerate(file_list, start=0):\n",
    "                with rasterio.open(layer) as src1:\n",
    "                    dst.write_band(id+1, src1.read(1).astype('float32'))\n",
    "                    \n",
    "    else:\n",
    "        print('\"Stack.tif\" already exists')\n",
    "\n",
    "    return feat_list_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the stacked image and masks it 9 times for cloud cover 10-90%, resulting in 9 arrays and 9 tuples of row, col indices of the array cells with non-nan values. It might take too much memory to have all 9 arrays loaded though ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(path, img, pctl):\n",
    "    \"\"\"\n",
    "    Masks stacked image with cloudmask by converting cloudy values to NaN\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str \n",
    "        Path to image folder\n",
    "    img :str \n",
    "        Name of image file (without file extension)\n",
    "    pctl : list of int\n",
    "        List of integers of cloud cover percentages to mask image with (10, 20, 30, etc.)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    data : array\n",
    "        3D array identical to input stacked image but with cloudy pixels masked\n",
    "    data_ind : list?\n",
    "        List? of indices in 'data' where cloudy pixels were masked. Used for reconstructing the image later \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get local image\n",
    "    with rasterio.open(path + 'images/'+ img + '/stack/stack.tif', 'r') as ds:\n",
    "        data = ds.read()\n",
    "        data = data.transpose((1, -1, 0)) # Not sure why the rasterio.read output is originally (D, W, H)\n",
    "    \n",
    "    # load cloudmasks\n",
    "    cloudMaskDir = path+'clouds'\n",
    "    \n",
    "    cloudMask = np.load(cloudMaskDir+'/'+img+'_clouds.npy')\n",
    "    cloudMask = cloudMask < np.percentile(cloudMask, pctl)\n",
    "    \n",
    "    # Need to remove NaNs because any arithmetic operation involving an NaN will result in NaN\n",
    "    data[cloudMask] = -999999\n",
    "    \n",
    "    # Convert -999999 to None\n",
    "    data[data == -999999] = np.nan\n",
    "\n",
    "    # Get indices of non-nan values. These are the indices of the original image array\n",
    "    data_ind = np.where(~np.isnan(data[:,:,1]))\n",
    "        \n",
    "    return data, data_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainVal(data):\n",
    "    \n",
    "    HOLDOUT_FRACTION = 0.1\n",
    "\n",
    "    # Reshape into a single vector of pixels.\n",
    "    data_vector = data.reshape([data.shape[0] * data.shape[1], data.shape[2]])\n",
    "\n",
    "    # Remove NaNs\n",
    "    data_vector = data_vector[~np.isnan(data_vector).any(axis=1)]\n",
    "    data_vector.shape\n",
    "\n",
    "    # Select only the valid data and shuffle it.\n",
    "    # valid_data = data_vector[numpy.equal(data_vector[:,8], 1)]\n",
    "    # np.random.shuffle(data_vector)\n",
    "\n",
    "    # Hold out a fraction of the labeled data for validation.\n",
    "    training_size = int(data_vector.shape[0] * (1 - HOLDOUT_FRACTION))\n",
    "    training_data = data_vector[0:training_size,:]\n",
    "    validation_data = data_vector[training_size:-1,:]\n",
    "\n",
    "    # Compute per-band means and standard deviations of the input bands.\n",
    "    data_mean = training_data[:,0:14].mean(0)\n",
    "    data_std = training_data[:,0:14].std(0)\n",
    "    \n",
    "    return [data_vector, training_data, validation_data, data_mean, data_std, training_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN layer builder\n",
    "# Function to train CNN on image, save model, and return performance metrics\n",
    "def make_nn_layer(input, output_size):\n",
    "    input_size = input.get_shape().as_list()[1]\n",
    "    weights = tf.Variable(tf.truncated_normal(\n",
    "        [input_size, output_size],\n",
    "        stddev=1.0 / math.sqrt(float(input_size))))\n",
    "    biases = tf.Variable(tf.zeros([output_size]))\n",
    "    return tf.matmul(input, weights) + biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNtrainer(data_vector, training_data, validation_data, data_mean, data_std, model_path, img, pctl):\n",
    "\n",
    "    model_path = model_path+img+'/'+img+'_clouds_'+str(pctl)\n",
    "    model_name = img+'_clouds_'+str(pctl)\n",
    "    checkpoint_filename = model_name+'_checkpoint'\n",
    "    \n",
    "    # Make a new directory for the model\n",
    "    try:\n",
    "        os.mkdir(model_path)\n",
    "    except FileExistsError:\n",
    "        print('Model directory already exists')\n",
    "        \n",
    "#     tf.reset_default_graph()\n",
    "    \n",
    "    import time\n",
    "    def timer(start,end, formatted = True):\n",
    "        if formatted == True: # Returns full formated time in hours, minutes, seconds\n",
    "            hours, rem = divmod(end-start, 3600)\n",
    "            minutes, seconds = divmod(rem, 60)\n",
    "            return str(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "        else: # Returns minutes + fraction of minute\n",
    "            minutes, seconds = divmod(time.time() - start_time, 60)\n",
    "            seconds = seconds/60\n",
    "            minutes = minutes + seconds\n",
    "            return str(minutes)\n",
    "\n",
    "    # Had to alter some config and runoptions because kept running into OOM at last step during eval \n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "    config.gpu_options.allow_growth = True\n",
    "    run_options=tf.RunOptions(report_tensor_allocations_upon_oom=True)\n",
    "\n",
    "    flooded = feat_list_files.index('flooded')\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    NUM_INPUT_BANDS = 14\n",
    "    NUM_HIDDEN_1 = 15\n",
    "    NUM_HIDDEN_2 = 15\n",
    "    NUM_CLASSES = 2\n",
    "    BATCH_SIZE = 1000\n",
    "    NUM_BATCHES = 1000\n",
    "\n",
    "    input = tf.placeholder(tf.float32, shape=[None, NUM_INPUT_BANDS], name='input')\n",
    "    labels = tf.placeholder(tf.float32, shape=[None], name='labels')\n",
    "    \n",
    "    normalized = (input - data_mean) / data_std\n",
    "    hidden1 = tf.nn.tanh(make_nn_layer(normalized, NUM_HIDDEN_1), name='hidden1')\n",
    "    hidden2 = tf.nn.tanh(make_nn_layer(hidden1, NUM_HIDDEN_2), name='hidden2')\n",
    "    logits = make_nn_layer(hidden2, NUM_CLASSES)\n",
    "    outputs = tf.argmax(logits, 1, name='outputs')\n",
    "\n",
    "    int_labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = int_labels, name='xentropy')\n",
    "    train_step = tf.train.AdamOptimizer().minimize(cross_entropy) # should we minimize something else?\n",
    "\n",
    "    correct_prediction = tf.equal(outputs, int_labels, name='correct_prediction')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    mySaver = tf.train.Saver(save_relative_paths=True)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        sess.run(init_op, options=run_options)\n",
    "\n",
    "        training_dict = {\n",
    "            input: training_data[:,0:14],\n",
    "            labels: training_data[:,14],\n",
    "        }\n",
    "\n",
    "        validation_dict = {\n",
    "            input: validation_data[:,0:14],\n",
    "            labels: validation_data[:,14],\n",
    "        }\n",
    "\n",
    "        for i in range(NUM_BATCHES):\n",
    "            batch = training_data[np.random.choice(training_size, BATCH_SIZE, False),:]\n",
    "            train_step.run({input: batch[:,0:14], labels: batch[:,14]})\n",
    "\n",
    "            if i % 100 == 0 or i == NUM_BATCHES - 1:\n",
    "    #             print('Train acc. %.2f%%, val acc. %.2f%%, train recall %.2f%, val recall %.2f%, train precision %.2f%, val precision %.2f%, at step %d' \n",
    "                print('Train acc. %.2f%%, val acc. %.2f%%, at step %d' \n",
    "                      % (accuracy.eval(training_dict) * 100,\n",
    "                         accuracy.eval(validation_dict) * 100, \n",
    "                         i))\n",
    "                \n",
    "        output_data = outputs.eval({input: data_vector[:,0:14]})\n",
    "\n",
    "        # Save the model\n",
    "        mySaver.save(sess, model_path+'/'+model_name+'.ckpt', \n",
    "                    global_step = NUM_BATCHES)\n",
    "#                     latest_filename=checkpoint_filename)\n",
    "\n",
    "    print('CNN training runtime for ' + str(pctl) + '% cloud cover: ' + timer(start_time, time.time()))\n",
    "    \n",
    "    return output_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4101_LC08_027038_20131103', '4101_LC08_027038_20131103_2', '4101_LC08_027039_20131103', '4115_LC08_021033_20131227_1', '4115_LC08_021033_20131227_2', '4337_LC08_026038_20160325_1', 'clear_4337_LC08_026038_20160325_1', 'clouds_4337_LC08_026038_20160325_1']\n"
     ]
    }
   ],
   "source": [
    "#===================================\n",
    "# Get list of images to train CNN on\n",
    "\n",
    "import os\n",
    "\n",
    "path = 'C:/Users/ipdavies/CPR/data/'\n",
    "\n",
    "# Get list of all images\n",
    "img_list = []\n",
    "for file in os.listdir(path+'images'):\n",
    "        img_list.append(file)\n",
    "\n",
    "print(img_list)\n",
    "\n",
    "# Manually creating list of images\n",
    "img_list = ['4101_LC08_027038_20131103', '4101_LC08_027038_20131103_2', '4101_LC08_027039_20131103',\n",
    "           '4115_LC08_021033_20131227_1', '4115_LC08_021033_20131227_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = ['4101_LC08_027038_20131103_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate cloud cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cloud_generation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f9014e3f08b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcloud_generation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcloudGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cloud_generation'"
     ]
    }
   ],
   "source": [
    "from cloud_generation import cloudGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating \"stack.tif\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\rasterio\\__init__.py:160: FutureWarning: GDAL-style transforms are deprecated and will not be supported in Rasterio 1.0.\n",
      "  transform = guard_transform(transform)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack directory already exists\n",
      "Removing existing \"stack.tif\" and creating new one\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/ipdavies/CPR/data/clouds/4101_LC08_027038_20131103_1_clouds.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-858f09550482>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpctl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpctls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpctl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mdata_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainVal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f9f2d01dd281>\u001b[0m in \u001b[0;36mpreprocessing\u001b[1;34m(path, img, pctl)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mcloudMaskDir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'clouds'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mcloudMask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloudMaskDir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_clouds.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mcloudMask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcloudMask\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloudMask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpctl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/ipdavies/CPR/data/clouds/4101_LC08_027038_20131103_1_clouds.npy'"
     ]
    }
   ],
   "source": [
    "# Run all of the functions: tif stack, preprocessing, CNN training, model and validation metric export\n",
    "\n",
    "path = 'C:/Users/ipdavies/CPR/data/'\n",
    "\n",
    "# Order in which features should be stacked to create stacked tif\n",
    "feat_list_new = ['aspect','curve', 'developed', 'distExtent', 'elevation', 'forest',\n",
    " 'GSW_maxExtent', 'hand', 'other_landcover', 'planted', 'slope', 'spi', 'twi', 'wetlands', 'flooded']\n",
    "\n",
    "# Directory with trained CNNs\n",
    "model_path = path+'models/cnn_vary_clouds/'\n",
    "\n",
    "# Cloud cover %s to use\n",
    "pctls = [10,20,30,40,50,60,70,80,90]\n",
    "\n",
    "for j, img in enumerate(img_list):\n",
    "\n",
    "    #  Stack all the flood imagery\n",
    "    feat_list_files = tifStacker(path, img, feat_list_new)\n",
    "\n",
    "    #  Stack all the flood imagery\n",
    "    feat_list_files = tifStacker(path, img, feat_list_new)\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    accuracy = []\n",
    "    times = []\n",
    "\n",
    "    # Load stacked image, preprocess\n",
    "    for i, pctl in enumerate(pctls):\n",
    "\n",
    "        data, data_ind = preprocessing(path, img, pctl)\n",
    "\n",
    "        data_vector, training_data, validation_data, data_mean, data_std, training_size = trainVal(data)\n",
    "\n",
    "        start_time = time.time() # Start timer for CNN training\n",
    "\n",
    "        print(img)\n",
    "        \n",
    "        y_pred = CNNtrainer(data_vector, training_data, validation_data, data_mean, data_std, model_path, img, pctl)\n",
    "\n",
    "        times.append(timer(start_time, time.time(), False)) # Elapsed time in minutes\n",
    "\n",
    "        y_true = data_vector[:,14]\n",
    "\n",
    "        precision.append(sklearn.metrics.precision_score(y_true, y_pred))\n",
    "        recall.append(sklearn.metrics.recall_score(y_true, y_pred))\n",
    "        f1.append(sklearn.metrics.f1_score(y_true, y_pred))\n",
    "        accuracy.append(sklearn.metrics.accuracy_score(y_true, y_pred))\n",
    "\n",
    "    valMetrics = pd.DataFrame(np.column_stack([pctls, accuracy, precision, recall, f1, times]),\n",
    "                          columns=['cloud_cover','accuracy','precision','recall','f1', 'time'])\n",
    "    \n",
    "    valMetrics.to_csv(model_path+img+'/valMetrics.csv', index=False)\n",
    "\n",
    "    valMetrics.plot(x='cloud_cover', y=['recall', 'precision','f1','accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
