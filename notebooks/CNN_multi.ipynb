{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the performance of CNNs on different flood images, cloud cover %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ee\n",
    "from IPython import display\n",
    "# import math\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "# from osgeo import gdal\n",
    "# import tempfile\n",
    "import tensorflow as tf\n",
    "# import urllib\n",
    "import rasterio\n",
    "from zipfile import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_clear = 'C:/Users/ipdavies/CPR/data/images/clear_4337_LC08_026038_20160325'\n",
    "# path_clouds = 'C:/Users/ipdavies/CPR/data/images/clouds_4337_LC08_026038_20160325'\n",
    "# model_name = 'cnn2'\n",
    "# model_path = 'C:/Users/ipdavies/CPR/data/models/'+model_name+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'C:/Users/ipdavies/CPR/data/images/4337_LC08_026038_20160325_1/4337_LC08_026038_20160325_1.zip'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN on an image with 10-90% cloud cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'cloudmasks', 'images', 'models', 'tables']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = 'C:/Users/ipdavies/CPR/data/'\n",
    "\n",
    "# Get list of all images\n",
    "img_list = []\n",
    "for file in os.listdir(path):\n",
    "        img_list.append(file)\n",
    "\n",
    "print(img_list)\n",
    "\n",
    "img = '4337_LC08_026038_20160325_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorders the tifs (i.e. individual bands) downloaded from GEE according to feature order in feat_list_new,\n",
    "# then stacks them all into one multiband image called 'stack.tif' located in input path\n",
    "# Reqs: rasterio, os, from zipfile import *\n",
    "# Ideally want to have this function in another notebook and call it, but running into problems - ZipFile not found\n",
    "# from ipynb.fs.full.useful_funcs import tifStacker\n",
    "\n",
    "def tifStacker(path, img, feat_list_new): \n",
    "\n",
    "    file_list = []\n",
    "    path = path+'images/'+img\n",
    "    \n",
    "    # This gets the name of all files in the zip folder, and formats them into a full path readable by rasterio.open()\n",
    "    with ZipFile(path + '/' + img + '.zip', 'r') as f:\n",
    "        names = f.namelist()\n",
    "        names = ['zip://'+ path + '/' + img + '.zip!' +name for name in names]\n",
    "        for file in names:\n",
    "            if file.endswith('.tif'):\n",
    "                file_list.append(file)\n",
    "    \n",
    "    feat_list_files = list(map(lambda x: x.split('.')[-2], file_list)) # Grabs a list of features in file order        \n",
    "    \n",
    "    # Create 1 row df of file names where each col is a feature name, in the order files are stored locally\n",
    "    file_arr = pd.DataFrame(data=[file_list], columns=feat_list_files)\n",
    "\n",
    "    # Then index the file list by the ordered list of feature names used in training\n",
    "    file_arr = file_arr.loc[:, feat_list_new]\n",
    "\n",
    "    # The take this re-ordered row as a list - the new file_list\n",
    "    file_list = list(file_arr.iloc[0,:])\n",
    "    \n",
    "    # Read metadata of first file. This needs to be a band in float32 dtype, because it sets the metadata for the entire stack\n",
    "    # and we are converting the other bands to float64\n",
    "    with rasterio.open(file_list[1]) as src0:\n",
    "        meta = src0.meta\n",
    "        meta['dtype'] = 'float32'\n",
    "    #         print(meta)\n",
    "\n",
    "    # Update meta to reflect the number of layers\n",
    "    meta.update(count = len(file_list))\n",
    "\n",
    "    # Read each layer, convert to float, and write it to stack\n",
    "    # There's also a gdal way to do this, but unsure how to convert to float: https://gis.stackexchange.com/questions/223910/using-rasterio-or-gdal-to-stack-multiple-bands-without-using-subprocess-commands\n",
    "\n",
    "    # Make new directory for stacked tif if it doesn't already exist\n",
    "    try:\n",
    "        os.mkdir(path +'/stack')\n",
    "    except FileExistsError:\n",
    "        print('Stack directory already exists')\n",
    "\n",
    "    # Remove stack file if already exists\n",
    "    try:\n",
    "        os.remove(path + '/stack/stack.tif')\n",
    "        print('Removing existing \"stack.tif\" and creating new one')\n",
    "    except FileNotFoundError:\n",
    "    #     pass\n",
    "        print('Creating \"stack.tif\"')\n",
    "\n",
    "    with rasterio.open(path + '/stack/stack.tif', 'w', **meta) as dst:\n",
    "        for id, layer in enumerate(file_list, start=0):\n",
    "            with rasterio.open(layer) as src1:\n",
    "                dst.write_band(id+1, src1.read(1).astype('float32'))\n",
    "    \n",
    "    return feat_list_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the stacked image and masks it 9 times for cloud cover 10-90%, resulting in 9 arrays and 9 tuples of row, col indices of the array cells with non-nan values. It might take too much memory to have all 9 arrays loaded though ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(path, img, pctl):\n",
    "\n",
    "    # Get local image\n",
    "    with rasterio.open(path + 'images/'+ img + '/stack/stack.tif', 'r') as ds:\n",
    "        data = ds.read()\n",
    "        data = data.transpose((1, -1, 0)) # Not sure why the rasterio.read output is originally (D, W, H)\n",
    "    \n",
    "    # load cloudmasks\n",
    "    cloudMaskDir = path+'cloudmasks/'+img\n",
    "    \n",
    "    cloudMask = np.load(cloudMaskDir+'/'+img+'_clouds_'+str(pctl)+'.npy')\n",
    "    \n",
    "    # Need to remove NaNs because any arithmetic operation involving an NaN will result in NaN\n",
    "    data[cloudMask] = -999999\n",
    "    \n",
    "    # Convert -999999 to None\n",
    "    data[data == -999999] = np.nan\n",
    "\n",
    "    # Get indices of non-nan values. These are the indices of the original image array\n",
    "    data_ind = np.where(~np.isnan(data[:,:,1]))\n",
    "        \n",
    "    return data, data_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainVal(data):\n",
    "    \n",
    "    HOLDOUT_FRACTION = 0.1\n",
    "\n",
    "    # Reshape into a single vector of pixels.\n",
    "    data_vector = data.reshape([data.shape[0] * data.shape[1], data.shape[2]])\n",
    "\n",
    "    # Remove NaNs\n",
    "    data_vector = data_vector[~np.isnan(data_vector).any(axis=1)]\n",
    "    data_vector.shape\n",
    "\n",
    "    # Select only the valid data and shuffle it.\n",
    "    # valid_data = data_vector[numpy.equal(data_vector[:,8], 1)]\n",
    "    # np.random.shuffle(data_vector)\n",
    "\n",
    "    # Hold out a fraction of the labeled data for validation.\n",
    "    training_size = int(data_vector.shape[0] * (1 - HOLDOUT_FRACTION))\n",
    "    training_data = data_vector[0:training_size,:]\n",
    "    validation_data = data_vector[training_size:-1,:]\n",
    "\n",
    "    # Compute per-band means and standard deviations of the input bands.\n",
    "    data_mean = training_data[:,0:14].mean(0)\n",
    "    data_std = training_data[:,0:14].std(0)\n",
    "    \n",
    "    return [data_vector, training_data, validation_data, data_mean, data_std, training_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train CNN on image, save model, and return performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN layer builder\n",
    "def make_nn_layer(input, output_size):\n",
    "    input_size = input.get_shape().as_list()[1]\n",
    "    weights = tf.Variable(tf.truncated_normal(\n",
    "        [input_size, output_size],\n",
    "        stddev=1.0 / math.sqrt(float(input_size))))\n",
    "    biases = tf.Variable(tf.zeros([output_size]))\n",
    "    return tf.matmul(input, weights) + biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNtrainer(data_vector, training_data, validation_data, data_mean, data_std, model_path, img, pctl):\n",
    "\n",
    "    model_path = model_path+img+'_clouds_'+str(pctl)\n",
    "    model_name = img+'_clouds_'+str(pctl)\n",
    "    checkpoint_filename = model_name+'_'+str(pctl)+'_checkpoint'\n",
    "    \n",
    "    # Make a new directory for the model\n",
    "    try:\n",
    "        os.mkdir(model_path)\n",
    "    except FileExistsError:\n",
    "        print('Model directory already exists')\n",
    "        \n",
    "#     tf.reset_default_graph()\n",
    "    \n",
    "    import time\n",
    "    def timer(start,end):\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        return str(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "\n",
    "    # Had to alter some config and runoptions because kept running into OOM at last step during eval \n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "    config.gpu_options.allow_growth = True\n",
    "    run_options=tf.RunOptions(report_tensor_allocations_upon_oom=True)\n",
    "\n",
    "    flooded = feat_list_files.index('flooded')\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    NUM_INPUT_BANDS = 14\n",
    "    NUM_HIDDEN_1 = 15\n",
    "    NUM_HIDDEN_2 = 15\n",
    "    NUM_CLASSES = 2\n",
    "    BATCH_SIZE = 1000\n",
    "    NUM_BATCHES = 1000\n",
    "\n",
    "    input = tf.placeholder(tf.float32, shape=[None, NUM_INPUT_BANDS])\n",
    "    labels = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "    normalized = (input - data_mean) / data_std\n",
    "    hidden1 = tf.nn.tanh(make_nn_layer(normalized, NUM_HIDDEN_1))\n",
    "    hidden2 = tf.nn.tanh(make_nn_layer(hidden1, NUM_HIDDEN_2))\n",
    "    logits = make_nn_layer(hidden2, NUM_CLASSES)\n",
    "    outputs = tf.argmax(logits, 1)\n",
    "\n",
    "    int_labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = int_labels, name='xentropy')\n",
    "    train_step = tf.train.AdamOptimizer().minimize(cross_entropy) # should we minimize something else?\n",
    "\n",
    "    correct_prediction = tf.equal(outputs, int_labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    mySaver = tf.train.Saver()\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        sess.run(init_op, options=run_options)\n",
    "\n",
    "        training_dict = {\n",
    "            input: training_data[:,0:14],\n",
    "            labels: training_data[:,14],\n",
    "        }\n",
    "\n",
    "        validation_dict = {\n",
    "            input: validation_data[:,0:14],\n",
    "            labels: validation_data[:,14],\n",
    "        }\n",
    "\n",
    "        for i in range(NUM_BATCHES):\n",
    "            batch = training_data[np.random.choice(training_size, BATCH_SIZE, False),:]\n",
    "            train_step.run({input: batch[:,0:14], labels: batch[:,14]})\n",
    "\n",
    "            if i % 100 == 0 or i == NUM_BATCHES - 1:\n",
    "    #             print('Train acc. %.2f%%, val acc. %.2f%%, train recall %.2f%, val recall %.2f%, train precision %.2f%, val precision %.2f%, at step %d' \n",
    "                print('Train acc. %.2f%%, val acc. %.2f%%, at step %d' \n",
    "                      % (accuracy.eval(training_dict) * 100,\n",
    "                         accuracy.eval(validation_dict) * 100, \n",
    "                         i))\n",
    "                \n",
    "        output_data = outputs.eval({input: data_vector[:,0:14]})\n",
    "\n",
    "        # Save the model\n",
    "        mySaver.save(sess, model_path+model_name+'.ckpt', \n",
    "                    global_step = NUM_BATCHES,\n",
    "                    latest_filename=checkpoint_filename)\n",
    "\n",
    "    print('CNN training runtime for ' + str(pctl) + '% cloud cover: ' + timer(start_time, time.time()))\n",
    "    \n",
    "    return output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack directory already exists\n",
      "Removing existing \"stack.tif\" and creating new one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\rasterio\\__init__.py:160: FutureWarning: GDAL-style transforms are deprecated and will not be supported in Rasterio 1.0.\n",
      "  transform = guard_transform(transform)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc. 45.11%, val acc. 37.99%, at step 0\n",
      "Train acc. 96.90%, val acc. 97.68%, at step 100\n",
      "Train acc. 98.15%, val acc. 99.04%, at step 200\n",
      "Train acc. 98.88%, val acc. 99.42%, at step 300\n",
      "Train acc. 98.90%, val acc. 99.42%, at step 400\n",
      "Train acc. 98.91%, val acc. 99.42%, at step 500\n",
      "Train acc. 98.92%, val acc. 99.42%, at step 600\n",
      "Train acc. 98.92%, val acc. 99.42%, at step 700\n",
      "Train acc. 98.93%, val acc. 99.42%, at step 800\n",
      "Train acc. 98.94%, val acc. 99.42%, at step 900\n",
      "Train acc. 98.95%, val acc. 99.44%, at step 999\n",
      "CNN training runtime for 10% cloud cover: 00:17:55.29\n",
      "Train acc. 54.19%, val acc. 43.44%, at step 0\n",
      "Train acc. 97.88%, val acc. 98.71%, at step 100\n",
      "Train acc. 97.94%, val acc. 98.73%, at step 200\n",
      "Train acc. 98.70%, val acc. 99.39%, at step 300\n",
      "Train acc. 98.79%, val acc. 99.43%, at step 400\n",
      "Train acc. 98.90%, val acc. 99.45%, at step 500\n",
      "Train acc. 98.92%, val acc. 99.45%, at step 600\n",
      "Train acc. 98.94%, val acc. 99.46%, at step 700\n",
      "Train acc. 98.95%, val acc. 99.47%, at step 800\n",
      "Train acc. 98.95%, val acc. 99.44%, at step 900\n",
      "Train acc. 98.97%, val acc. 99.47%, at step 999\n",
      "CNN training runtime for 20% cloud cover: 00:15:53.92\n",
      "Train acc. 55.46%, val acc. 55.53%, at step 0\n",
      "Train acc. 97.79%, val acc. 98.43%, at step 100\n",
      "Train acc. 98.34%, val acc. 99.00%, at step 200\n",
      "Train acc. 98.84%, val acc. 99.42%, at step 300\n",
      "Train acc. 98.89%, val acc. 99.46%, at step 400\n",
      "Train acc. 98.90%, val acc. 99.45%, at step 500\n",
      "Train acc. 98.90%, val acc. 99.45%, at step 600\n",
      "Train acc. 98.91%, val acc. 99.46%, at step 700\n",
      "Train acc. 98.91%, val acc. 99.46%, at step 800\n",
      "Train acc. 98.91%, val acc. 99.45%, at step 900\n",
      "Train acc. 98.94%, val acc. 99.47%, at step 999\n",
      "CNN training runtime for 30% cloud cover: 00:13:53.29\n",
      "Train acc. 56.40%, val acc. 49.38%, at step 0\n",
      "Train acc. 98.23%, val acc. 98.88%, at step 100\n",
      "Train acc. 98.27%, val acc. 98.90%, at step 200\n",
      "Train acc. 98.76%, val acc. 99.32%, at step 300\n",
      "Train acc. 98.79%, val acc. 99.40%, at step 400\n",
      "Train acc. 98.80%, val acc. 99.36%, at step 500\n",
      "Train acc. 98.83%, val acc. 99.42%, at step 600\n",
      "Train acc. 98.84%, val acc. 99.43%, at step 700\n",
      "Train acc. 98.85%, val acc. 99.42%, at step 800\n",
      "Train acc. 98.86%, val acc. 99.44%, at step 900\n",
      "Train acc. 98.86%, val acc. 99.45%, at step 999\n",
      "CNN training runtime for 40% cloud cover: 00:10:53.63\n",
      "Train acc. 56.57%, val acc. 50.84%, at step 0\n",
      "Train acc. 98.20%, val acc. 98.90%, at step 100\n",
      "Train acc. 98.29%, val acc. 98.91%, at step 200\n",
      "Train acc. 98.80%, val acc. 99.38%, at step 300\n",
      "Train acc. 98.83%, val acc. 99.43%, at step 400\n",
      "Train acc. 98.86%, val acc. 99.48%, at step 500\n",
      "Train acc. 98.85%, val acc. 99.47%, at step 600\n",
      "Train acc. 98.88%, val acc. 99.52%, at step 700\n",
      "Train acc. 98.89%, val acc. 99.52%, at step 800\n",
      "Train acc. 98.87%, val acc. 99.50%, at step 900\n",
      "Train acc. 98.89%, val acc. 99.53%, at step 999\n",
      "CNN training runtime for 50% cloud cover: 00:09:05.49\n",
      "Train acc. 47.46%, val acc. 54.25%, at step 0\n",
      "Train acc. 96.74%, val acc. 97.90%, at step 100\n",
      "Train acc. 98.64%, val acc. 99.34%, at step 200\n",
      "Train acc. 98.83%, val acc. 99.47%, at step 300\n",
      "Train acc. 98.86%, val acc. 99.49%, at step 400\n",
      "Train acc. 98.88%, val acc. 99.50%, at step 500\n",
      "Train acc. 98.89%, val acc. 99.50%, at step 600\n",
      "Train acc. 98.90%, val acc. 99.52%, at step 700\n",
      "Train acc. 98.90%, val acc. 99.52%, at step 800\n",
      "Train acc. 98.91%, val acc. 99.52%, at step 900\n",
      "Train acc. 98.92%, val acc. 99.53%, at step 999\n",
      "CNN training runtime for 60% cloud cover: 00:06:27.45\n",
      "Train acc. 56.67%, val acc. 61.07%, at step 0\n",
      "Train acc. 97.83%, val acc. 98.62%, at step 100\n",
      "Train acc. 98.02%, val acc. 98.68%, at step 200\n",
      "Train acc. 98.89%, val acc. 99.49%, at step 300\n",
      "Train acc. 98.94%, val acc. 99.51%, at step 400\n",
      "Train acc. 98.97%, val acc. 99.52%, at step 500\n",
      "Train acc. 98.99%, val acc. 99.52%, at step 600\n",
      "Train acc. 98.98%, val acc. 99.52%, at step 700\n",
      "Train acc. 99.00%, val acc. 99.54%, at step 800\n",
      "Train acc. 99.01%, val acc. 99.53%, at step 900\n",
      "Train acc. 99.03%, val acc. 99.56%, at step 999\n",
      "CNN training runtime for 70% cloud cover: 00:04:17.28\n",
      "Train acc. 51.16%, val acc. 43.95%, at step 0\n",
      "Train acc. 96.75%, val acc. 97.67%, at step 100\n",
      "Train acc. 98.61%, val acc. 98.97%, at step 200\n",
      "Train acc. 98.93%, val acc. 99.57%, at step 300\n",
      "Train acc. 98.99%, val acc. 99.61%, at step 400\n",
      "Train acc. 99.01%, val acc. 99.59%, at step 500\n",
      "Train acc. 99.02%, val acc. 99.58%, at step 600\n",
      "Train acc. 99.04%, val acc. 99.59%, at step 700\n",
      "Train acc. 99.05%, val acc. 99.60%, at step 800\n",
      "Train acc. 99.05%, val acc. 99.59%, at step 900\n",
      "Train acc. 99.07%, val acc. 99.62%, at step 999\n",
      "CNN training runtime for 80% cloud cover: 00:02:35.98\n",
      "Train acc. 42.82%, val acc. 44.37%, at step 0\n",
      "Train acc. 97.64%, val acc. 98.81%, at step 100\n",
      "Train acc. 98.76%, val acc. 99.66%, at step 200\n",
      "Train acc. 98.93%, val acc. 99.67%, at step 300\n",
      "Train acc. 99.07%, val acc. 99.70%, at step 400\n",
      "Train acc. 99.15%, val acc. 99.74%, at step 500\n",
      "Train acc. 99.16%, val acc. 99.75%, at step 600\n",
      "Train acc. 99.18%, val acc. 99.75%, at step 700\n",
      "Train acc. 99.19%, val acc. 99.75%, at step 800\n",
      "Train acc. 99.19%, val acc. 99.75%, at step 900\n",
      "Train acc. 99.20%, val acc. 99.75%, at step 999\n",
      "CNN training runtime for 90% cloud cover: 00:01:13.27\n",
      "   cloud_cover  accuracy  precision    recall        f1         time\n",
      "0         10.0  0.990030   0.943018  0.825945  0.880608  1075.601589\n",
      "1         20.0  0.990154   0.929023  0.839604  0.882053   954.268305\n",
      "2         30.0  0.989906   0.929410  0.834826  0.879583   833.606992\n",
      "3         40.0  0.989151   0.923619  0.822765  0.870280   654.042703\n",
      "4         50.0  0.989532   0.944438  0.801385  0.867050   545.968620\n",
      "5         60.0  0.989774   0.926598  0.808865  0.863738   387.938580\n",
      "6         70.0  0.990819   0.927490  0.822126  0.871635   257.750089\n",
      "7         80.0  0.991255   0.907706  0.807936  0.854920   156.312802\n",
      "8         90.0  0.992570   0.881362  0.755624  0.813664    73.662137\n"
     ]
    }
   ],
   "source": [
    "# Loop to do all the functions together\n",
    "\n",
    "import math\n",
    "from math import sqrt\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, accuracy_score\n",
    "import time\n",
    "\n",
    "path = 'C:/Users/ipdavies/CPR/data/'\n",
    "feat_list_new = ['aspect','curve', 'developed', 'distExtent', 'elevation', 'forest',\n",
    " 'GSW_maxExtent', 'hand', 'other_landcover', 'planted', 'slope', 'spi', 'twi', 'wetlands', 'flooded']\n",
    "model_path = path+'models/cnn_vary_clouds/'\n",
    "img = '4337_LC08_026038_20160325_1'\n",
    "pctls = [10,20,30,40,50,60,70,80,90]\n",
    "\n",
    "#  Stack all the flood imagery\n",
    "feat_list_files = tifStacker(path, img, feat_list_new)\n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "accuracy = []\n",
    "times = []\n",
    "\n",
    "# Load stacked image, preprocess\n",
    "for i, pctl in enumerate(pctls):\n",
    "    \n",
    "    data, data_ind = preprocessing(path, img, pctl)\n",
    "    \n",
    "    data_vector, training_data, validation_data, data_mean, data_std, training_size = trainVal(data)\n",
    "    \n",
    "    start_time = time.time() # Start timer for CNN training\n",
    "    \n",
    "    y_pred = CNNtrainer(data_vector, training_data, validation_data, data_mean, data_std, model_path, img, pctl)\n",
    "    \n",
    "    times.append(time.time() - start_time) # Elapsed time in seconds\n",
    "    \n",
    "    y_true = data_vector[:,14]\n",
    "    \n",
    "    precision.append(sklearn.metrics.precision_score(y_true, y_pred))\n",
    "    recall.append(sklearn.metrics.recall_score(y_true, y_pred))\n",
    "    f1.append(sklearn.metrics.f1_score(y_true, y_pred))\n",
    "    accuracy.append(sklearn.metrics.accuracy_score(y_true, y_pred))\n",
    "    \n",
    "metrics = pd.DataFrame(np.column_stack([pctls, accuracy, precision, recall, f1, times]),\n",
    "                      columns=['cloud_cover','accuracy','precision','recall','f1', 'time'])\n",
    "\n",
    "print(metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to add a printout or metric with how many flooded pixels are in train/test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.to_csv(path+'tables/CNN_cloud_metrics/'+'train_'+img+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x187ea426710>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training metrics\n",
    "\n",
    "metrics.plot(x='cloud_cover', y=['recall', 'precision','f1','accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on cloud gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_gaps(path, img, pctl):\n",
    "\n",
    "    # Get local image\n",
    "    with rasterio.open(path + 'images/'+ img + '/stack/stack.tif', 'r') as ds:\n",
    "        data = ds.read()\n",
    "        data = data.transpose((1, -1, 0)) # Not sure why the rasterio.read output is originally (D, W, H)\n",
    "    \n",
    "    # load cloudmasks\n",
    "    cloudMaskDir = path+'cloudmasks/'+img\n",
    "    \n",
    "    cloudMask = np.load(cloudMaskDir+'/'+img+'_clouds_'+str(pctl)+'.npy')\n",
    "    \n",
    "    # Invert cloudmask to get the gaps\n",
    "    cloudMask = np.invert(cloudMask)\n",
    "    \n",
    "    # Need to remove NaNs because any arithmetic operation involving an NaN will result in NaN\n",
    "    data[cloudMask] = -999999\n",
    "    \n",
    "    # Convert -999999 to None\n",
    "    data[data == -999999] = np.nan\n",
    "\n",
    "    # Get indices of non-nan values. These are the indices of the original image array\n",
    "    data_ind = np.where(~np.isnan(data[:,:,1]))\n",
    "    \n",
    "    # Reshape into a single vector of pixels.\n",
    "    data_vector = data.reshape([data.shape[0] * data.shape[1], data.shape[2]])\n",
    "\n",
    "    # Remove NaNs\n",
    "    data_vector = data_vector[~np.isnan(data_vector).any(axis=1)]\n",
    "\n",
    "    # Compute per-band means and standard deviations of the input bands.\n",
    "    data_mean = training_data[:,0:14].mean(0)\n",
    "    data_std = training_data[:,0:14].std(0)\n",
    "\n",
    "    return data_vector, data_mean, data_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: TF doesn't save every variable, like outputs or input, so we need to save those somehow\n",
    "https://stackoverflow.com/questions/43887425/how-to-import-a-model-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/ipdavies/CPR/data/models/cnn_vary_clouds/4337_LC08_026038_20160325_1_clouds_10/4337_LC08_026038_20160325_1_clouds_104337_LC08_026038_20160325_1_clouds_10.ckpt-1000.meta'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'C:/Users/ipdavies/CPR/data/'\n",
    "model_path = path+'models/cnn_vary_clouds/'\n",
    "model_path = model_path+img+'_clouds_'+str(pctl)\n",
    "model_name = img+'_clouds_'+str(pctl)\n",
    "# model_name+'_'+str(pctl)+'_checkpoint'\n",
    "# model_path\n",
    "model_path+'/'+model_name+model_name+'.ckpt-1000.meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gapFill(data_vector, data_mean, data_std, img, pctl, model_path):\n",
    "    \n",
    "    model_path = model_path+img+'_clouds_'+str(pctl)\n",
    "    model_name = img+'_clouds_'+str(pctl)\n",
    "    checkpoint_filename = model_name+'_'+str(pctl)+'_checkpoint'\n",
    "    \n",
    "    # Had to alter some config and runoptions because kept running into OOM at last step during eval \n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "\n",
    "    config.gpu_options.allow_growth = True\n",
    "#     sess = tf.Session(config=config)\n",
    "    # run_options = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "    run_options=tf.RunOptions(report_tensor_allocations_upon_oom=True)\n",
    "\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer(), options=run_options)\n",
    "#         mySaver = tf.train.import_meta_graph(model_path+'/'+model_name+'.ckpt-1000.meta')\n",
    "        mySaver = tf.train.import_meta_graph(model_path+'/'+model_name+model_name+'.ckpt-1000.meta')\n",
    "        mySaver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir=model_path+'./'))#, latest_filename=checkpoint_filename))\n",
    "#         outputs=tf.get_collection('outputs')\n",
    "\n",
    "        y_pred = outputs.eval(session=sess, feed_dict ={input: data_vector[:,0:14]})\n",
    "\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\ipdavies\\CPR\\data\\models\\cnn_vary_clouds\\4337_LC08_026038_20160325_1_clouds_10\\4337_LC08_026038_20160325_1_clouds_10_10_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't load save_path when it is None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-59f26eb590fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'models/cnn_vary_clouds/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgapFill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-78-cbf7ff099260>\u001b[0m in \u001b[0;36mgapFill\u001b[1;34m(data_vector, data_mean, data_std, img, pctl, model_path)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#         mySaver = tf.train.import_meta_graph(model_path+'/'+model_name+'.ckpt-1000.meta')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mmySaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.ckpt-1000.meta'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mmySaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/4337_LC08_026038_20160325_1_clouds_90_90_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'./'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, latest_filename=checkpoint_filename))\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;31m#         outputs=tf.get_collection('outputs')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1743\u001b[0m       \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1746\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Can't load save_path when it is None."
     ]
    }
   ],
   "source": [
    "model_path = path+'models/cnn_vary_clouds/'\n",
    "y_pred = gapFill(data_vector, data_mean, data_std, img, 90, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT_BANDS = 14\n",
    "NUM_HIDDEN_1 = 15\n",
    "NUM_HIDDEN_2 = 15\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Prepare feed dictionary\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape=[None, NUM_INPUT_BANDS])\n",
    "labels = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "normalized = (input - data_mean) / data_std\n",
    "\n",
    "hidden1 = tf.nn.tanh(make_nn_layer(normalized, NUM_HIDDEN_1))\n",
    "hidden2 = tf.nn.tanh(make_nn_layer(hidden1, NUM_HIDDEN_2))\n",
    "logits = make_nn_layer(hidden2, NUM_CLASSES)\n",
    "outputs = tf.argmax(logits, 1)\n",
    "\n",
    "int_labels = tf.to_int64(labels)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = int_labels, name='xentropy')\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy) # should we minimize something else?\n",
    "\n",
    "correct_prediction = tf.equal(outputs, int_labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:Couldn't match files for checkpoint C:/Users/ipdavies/CPR/data/models/cnn_vary_clouds/4337_LC08_026038_20160325_1_clouds_104337_LC08_026038_20160325_1_clouds_10.ckpt-1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't load save_path when it is None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-34256ef8c985>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdata_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing_gaps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpctl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgapFill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpctl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-f59f539e6967>\u001b[0m in \u001b[0;36mgapFill\u001b[1;34m(data_vector, data_mean, data_std, img, pctl, model_path)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mmySaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.ckpt-1000.meta'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#         mySaver.restore(sess, tf.train.load_checkpoint(model_path+checkpoint_filename))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mmySaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'./'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatest_filename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;31m#         outputs=tf.get_collection('outputs')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1743\u001b[0m       \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1746\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Can't load save_path when it is None."
     ]
    }
   ],
   "source": [
    "model_path = path+'models/cnn_vary_clouds/'\n",
    "pctls = [10,20,30,40,50,60,70,80,90]\n",
    "img = '4337_LC08_026038_20160325_1'\n",
    "pctls = [10,20,30,40,50,60,70,80,90]\n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "accuracy = []\n",
    "predictions = []\n",
    "\n",
    "for i, pctl in enumerate(pctls):\n",
    "    \n",
    "    data_vector, data_mean, data_std = preprocessing_gaps(path, img, pctl)\n",
    "    \n",
    "    y_pred = gapFill(data_vector, data_mean, data_std, img, pctl, model_path)\n",
    "    \n",
    "    y_true = data_vector[:,14]\n",
    "    \n",
    "    precision.append(sklearn.metrics.precision_score(y_true, y_pred))\n",
    "    recall.append(sklearn.metrics.recall_score(y_true, y_pred))\n",
    "    f1.append(sklearn.metrics.f1_score(y_true, y_pred))\n",
    "    predictions.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
