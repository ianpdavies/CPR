{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\thesis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\thesis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\thesis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\thesis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\thesis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\thesis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import math\n",
    "from zipfile import *\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ipdavies\\\\CPR\\\\notebooks\\\\uncertainty'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from CPR.utils import tifStacker, preprocessing, trainVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tifStacker(path, img, feat_list_new, overwrite=False): \n",
    "\n",
    "    \"\"\" Reorders the tifs (i.e. individual bands) downloaded from GEE according to feature order in feat_list_new, \n",
    "    then stacks them all into one multiband image called 'stack.tif' located in input path. Requires rasterio, \n",
    "    os, from zipfile import *\n",
    "    \n",
    "    Ideally want to have this function in another notebook and call it, but running into problems - ZipFile not found \n",
    "    \n",
    "    from ipynb.fs.full.useful_funcs import tifStacker\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str \n",
    "        Path to image folder\n",
    "    img :str \n",
    "        Name of image file (without file extension)\n",
    "    feat_list_new : list\n",
    "        List of feature names (str) to be the desired order of the output stacked .tif - target feature must be last\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    \"stacked.tif\" in 'path' location \n",
    "    feat_list_files : list \n",
    "        Not sure what that is or what it's for \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    file_list = []\n",
    "    imgPath = path+'images/'+img\n",
    "    \n",
    "    # This gets the name of all files in the zip folder, and formats them into a full path readable by rasterio.open()\n",
    "    with ZipFile(imgPath + '/' + img + '.zip', 'r') as f:\n",
    "        names = f.namelist()\n",
    "        names = ['zip://'+ imgPath + '/' + img + '.zip!' +name for name in names]\n",
    "        for file in names:\n",
    "            if file.endswith('.tif'):\n",
    "                file_list.append(file)\n",
    "    \n",
    "    feat_list_files = list(map(lambda x: x.split('.')[-2], file_list)) # Grabs a list of features in file order        \n",
    "    \n",
    "    if overwrite==False:\n",
    "            if os.path.exists(imgPath + '/stack/stack.tif') == True:\n",
    "                print('\"stack.tif\" already exists for '+ img)\n",
    "                return\n",
    "            else:\n",
    "                print('No existing \"stack.tif\" for '+img+', creating one')\n",
    "        \n",
    "    if overwrite==True:\n",
    "        # Remove stack file if already exists\n",
    "        try:\n",
    "            os.remove(imgPath + '/stack/stack.tif')\n",
    "            print('Removing existing \"stack.tif\" and creating new one')\n",
    "        except FileNotFoundError:\n",
    "            print('No existing \"stack.tif\" for '+img+', creating one')\n",
    "            \n",
    "    # Create 1 row df of file names where each col is a feature name, in the order files are stored locally\n",
    "    file_arr = pd.DataFrame(data=[file_list], columns=feat_list_files)\n",
    "\n",
    "    # Then index the file list by the ordered list of feature names used in training\n",
    "    file_arr = file_arr.loc[:, feat_list_new]\n",
    "\n",
    "    # The take this re-ordered row as a list - the new file_list\n",
    "    file_list = list(file_arr.iloc[0,:])\n",
    "\n",
    "    # Read metadata of first file. This needs to be a band in float32 dtype, because it sets the metadata for the entire stack\n",
    "    # and we are converting the other bands to float64\n",
    "    with rasterio.open(file_list[1]) as src0:\n",
    "        meta = src0.meta\n",
    "        meta['dtype'] = 'float32'\n",
    "    #         print(meta)\n",
    "\n",
    "    # Update meta to reflect the number of layers\n",
    "    meta.update(count = len(file_list))\n",
    "\n",
    "    # Read each layer, convert to float, and write it to stack\n",
    "    # There's also a gdal way to do this, but unsure how to convert to float: https://gis.stackexchange.com/questions/223910/using-rasterio-or-gdal-to-stack-multiple-bands-without-using-subprocess-commands\n",
    "\n",
    "    # Make new directory for stacked tif if it doesn't already exist\n",
    "    try:\n",
    "        os.mkdir(imgPath +'/stack')\n",
    "    except FileExistsError:\n",
    "        print('Stack directory already exists') \n",
    "\n",
    "    with rasterio.open(imgPath + '/stack/stack.tif', 'w', **meta) as dst:\n",
    "        for id, layer in enumerate(file_list, start=0):\n",
    "            with rasterio.open(layer) as src1:\n",
    "                dst.write_band(id+1, src1.read(1).astype('float32'))\n",
    "\n",
    "    return feat_list_files\n",
    "\n",
    "# =============================================================\n",
    "# =============================================================\n",
    "# =============================================================\n",
    "\n",
    "def preprocessing(path, img, pctl, gaps):\n",
    "    \"\"\"\n",
    "    Masks stacked image with cloudmask by converting cloudy values to NaN\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str \n",
    "        Path to image folder\n",
    "    img :str \n",
    "        Name of image file (without file extension)\n",
    "    pctl : list of int\n",
    "        List of integers of cloud cover percentages to mask image with (10, 20, 30, etc.)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    data : array\n",
    "        3D array identical to input stacked image but with cloudy pixels masked\n",
    "    data_ind : list?\n",
    "        List? of indices in 'data' where cloudy pixels were masked. Used for reconstructing the image later \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get local image\n",
    "    with rasterio.open(path + 'images/'+ img + '/stack/stack.tif', 'r') as ds:\n",
    "        data = ds.read()\n",
    "        data = data.transpose((1, -1, 0)) # Not sure why the rasterio.read output is originally (D, W, H)\n",
    "    \n",
    "    # load cloudmasks\n",
    "    cloudMaskDir = path+'clouds'\n",
    "    \n",
    "    cloudMask = np.load(cloudMaskDir+'/'+img+'_clouds.npy')\n",
    "    cloudMask = cloudMask < np.percentile(cloudMask, pctl)\n",
    "    \n",
    "    # Need to remove NaNs because any arithmetic operation involving an NaN will result in NaN\n",
    "    data[cloudMask] = -999999\n",
    "    \n",
    "    # Convert -999999 to None\n",
    "    data[data == -999999] = np.nan\n",
    "\n",
    "    # Get indices of non-nan values. These are the indices of the original image array\n",
    "    data_ind = np.where(~np.isnan(data[:,:,1]))\n",
    "        \n",
    "    return data, data_ind\n",
    "\n",
    "# =============================================================\n",
    "# =============================================================\n",
    "# =============================================================\n",
    "\n",
    "def trainVal(data):\n",
    "    \n",
    "    HOLDOUT_FRACTION = 0.1\n",
    "\n",
    "    # Reshape into a single vector of pixels.\n",
    "    data_vector = data.reshape([data.shape[0] * data.shape[1], data.shape[2]])\n",
    "\n",
    "    # Remove NaNs\n",
    "    data_vector = data_vector[~np.isnan(data_vector).any(axis=1)]\n",
    "    data_vector.shape\n",
    "\n",
    "    # Select only the valid data and shuffle it.\n",
    "    # valid_data = data_vector[numpy.equal(data_vector[:,8], 1)]\n",
    "    # np.random.shuffle(data_vector)\n",
    "\n",
    "    # Hold out a fraction of the labeled data for validation.\n",
    "    training_size = int(data_vector.shape[0] * (1 - HOLDOUT_FRACTION))\n",
    "    training_data = data_vector[0:training_size,:]\n",
    "    validation_data = data_vector[training_size:-1,:]\n",
    "\n",
    "    # Compute per-band means and standard deviations of the input bands.\n",
    "    data_mean = training_data[:,0:14].mean(0)\n",
    "    data_std = training_data[:,0:14].std(0)\n",
    "    \n",
    "    # Normalize data - only the non-binary variables\n",
    "    data_vector[:,0:14] = (data_vector[:,0:14] - data_mean) / data_std\n",
    "    \n",
    "    return [data_vector, training_data, validation_data, training_size]\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# =============================================================\n",
    "# =============================================================\n",
    "\n",
    "# Get test data from cloudy portions\n",
    "def preprocessing_gaps(path, img, pctl):\n",
    "\n",
    "    # Get local image\n",
    "    with rasterio.open(path + 'images/'+ img + '/stack/stack.tif', 'r') as ds:\n",
    "        data = ds.read()\n",
    "        data = data.transpose((1, -1, 0)) # Not sure why the rasterio.read output is originally (D, W, H)\n",
    "    \n",
    "    # load cloudmasks\n",
    "    cloudMaskDir = path+'clouds'\n",
    "    \n",
    "    cloudMask = np.load(cloudMaskDir+'/'+img+'_clouds.npy')\n",
    "    # Note how the sign is >=, not <, we want inverse of training data\n",
    "    cloudMask = cloudMask >= np.percentile(cloudMask, pctl)\n",
    "\n",
    "    # Need to remove NaNs because any arithmetic operation involving an NaN will result in NaN\n",
    "    data[cloudMask] = -999999\n",
    "    \n",
    "    # Convert -999999 to None\n",
    "    data[data == -999999] = np.nan\n",
    "\n",
    "    # Get indices of non-nan values. These are the indices of the original image array\n",
    "    data_ind = np.where(~np.isnan(data[:,:,1]))\n",
    "    \n",
    "    # Reshape into a single vector of pixels.\n",
    "    data_vector = data.reshape([data.shape[0] * data.shape[1], data.shape[2]])\n",
    "\n",
    "    # Remove NaNs\n",
    "    data_vector = data_vector[~np.isnan(data_vector).any(axis=1)]\n",
    "\n",
    "    # Compute per-band means and standard deviations of the input bands.\n",
    "    data_mean = data_vector[:,0:14].mean(0)\n",
    "    data_std = data_vector[:,0:14].std(0)\n",
    "\n",
    "    # Normalize features\n",
    "    data_vector[:,0:14] = (data_vector[:,0:14] - data_mean) / data_std\n",
    "    \n",
    "    return data_vector, data_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"stack.tif\" already exists for 4115_LC08_021033_20131227_test\n",
      "Cloud image already exists for 4115_LC08_021033_20131227_test\n"
     ]
    }
   ],
   "source": [
    "%run ../cloud_generation.ipynb # Gets cloudGenerator function from the other ipynb\n",
    "\n",
    "path = 'C:/Users/ipdavies/CPR/data/'\n",
    "\n",
    "# Order in which features should be stacked to create stacked tif\n",
    "feat_list_new = ['aspect','curve', 'developed', 'GSW_distExtent', 'elevation', 'forest',\n",
    " 'GSW_maxExtent', 'hand', 'other_landcover', 'planted', 'slope', 'spi', 'twi', 'wetlands', 'flooded']\n",
    "\n",
    "img_list = ['4115_LC08_021033_20131227_test']\n",
    "\n",
    "for j, img in enumerate(img_list):\n",
    "    #  Stack all the flood imagery\n",
    "    tifStacker(path, img, feat_list_new, overwrite=False)\n",
    "    cloudGenerator(img, path, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctl = [20]\n",
    "data, data_ind = preprocessing(path, img, pctl)\n",
    "data_vector, training_data, validation_data, training_size = trainVal(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try using UncertaintyNN\n",
    "from [here](https://github.com/hutec/UncertaintyNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_model(x, dropout_rate):\n",
    "    \"\"\"\n",
    "    Model that combines aleatoric and epistemic uncertainty.\n",
    "    Based on the \"What uncertainties do we need\" paper by Kendall.\n",
    "    Works for simple 2D data.\n",
    "    :param x: Input feature x\n",
    "    :param dropout_rate:\n",
    "    :return: prediction, log(sigma^2)\n",
    "    \"\"\"\n",
    "\n",
    "    # with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    fc1 = tf.layers.dense(inputs=x, units=50, activation=tf.nn.relu)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout_rate)\n",
    "\n",
    "    fc2 = tf.layers.dense(inputs=fc1, units=50, activation=tf.nn.relu)\n",
    "    fc2 = tf.nn.dropout(fc2, dropout_rate)\n",
    "\n",
    "    # Output layers has predictive mean and variance sigma^2\n",
    "    output_layer = tf.layers.dense(fc2, units=2)\n",
    "\n",
    "    predictions = tf.expand_dims(output_layer[:, 0], -1)\n",
    "    log_variance = tf.expand_dims(output_layer[:, 1], -1)\n",
    "\n",
    "    return predictions, log_variance\n",
    "\n",
    "def combined_training(x_truth, y_truth, dropout, learning_rate, epochs, display_step=2000):\n",
    "    \"\"\"\n",
    "    Generic training of a Combined (uncertainty) network for 2D data.\n",
    "    :param x_truth: training samples x\n",
    "    :param y_truth: training samples y / label\n",
    "    :param dropout:\n",
    "    :param learning_rate:\n",
    "    :param epochs:\n",
    "    :param display_step:\n",
    "    :return: session, x_placeholder, dropout_placeholder\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    x_placeholder = tf.placeholder(tf.float32, [None, 1])\n",
    "    y_placeholder = tf.placeholder(tf.float32, [None, 1])\n",
    "    dropout_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "    prediction, log_variance = combined_model(x_placeholder, dropout_placeholder)\n",
    "\n",
    "    tf.add_to_collection(\"prediction\", prediction)\n",
    "    tf.add_to_collection(\"log_variance\", log_variance)\n",
    "\n",
    "    loss = tf.reduce_sum(0.5 * tf.exp(-1 * log_variance) * tf.square(tf.abs(y_placeholder - prediction))\n",
    "                         + 0.5 * log_variance)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        feed_dict = {x_placeholder: x_truth.reshape([-1, 1]),\n",
    "                     y_placeholder: y_truth.reshape([-1, 1]),\n",
    "                     dropout_placeholder: dropout}\n",
    "\n",
    "        sess.run(train, feed_dict=feed_dict)\n",
    "\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch {}\".format(epoch))\n",
    "            current_loss = sess.run(loss, feed_dict=feed_dict)\n",
    "            print(\"Loss {}\".format(current_loss))\n",
    "            print(\"================\")\n",
    "\n",
    "    print(\"Training done\")\n",
    "\n",
    "    return sess, x_placeholder, dropout_placeholder\n",
    "\n",
    "\n",
    "\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "# from data import sample_generators\n",
    "# import plotting\n",
    "\n",
    "# def combined_evaluation(x, y, dropout, learning_rate, epochs, n_passes, ax):\n",
    "#     \"\"\"\n",
    "#     :param x:\n",
    "#     :param y:\n",
    "#     :param dropout:\n",
    "#     :param learning_rate:\n",
    "#     :param epochs:\n",
    "#     :param n_passes:\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     sess, x_placeholder, dropout_placeholder = \\\n",
    "#         combined_training(x, y, 0.2, learning_rate, epochs)\n",
    "\n",
    "#     prediction_op = sess.graph.get_collection(\"prediction\")\n",
    "#     log_variance = sess.graph.get_collection(\"log_variance\")\n",
    "#     aleatoric_op = tf.exp(log_variance)\n",
    "\n",
    "#     additional_range = 0.1 * np.max(x)\n",
    "#     x_eval = np.linspace(np.min(x) - additional_range, np.max(x) + additional_range, 100).reshape([-1, 1])\n",
    "\n",
    "#     feed_dict = {x_placeholder: x_eval,\n",
    "#                  dropout_placeholder: dropout}\n",
    "\n",
    "#     predictions = []\n",
    "#     aleatorics = []\n",
    "#     for _ in range(n_passes):\n",
    "#         prediction, aleatoric = sess.run([prediction_op, aleatoric_op], feed_dict)\n",
    "#         predictions.append(prediction[0])\n",
    "#         aleatorics.append(aleatoric[0])\n",
    "\n",
    "#     y_eval = np.mean(predictions, axis=0).flatten()\n",
    "#     epistemic_eval = np.var(predictions, axis=0).flatten()\n",
    "#     aleatoric_eval = np.mean(aleatorics, axis=0).flatten()\n",
    "#     total_uncertainty_eval = epistemic_eval + aleatoric_eval\n",
    "\n",
    "#     plotting.plot_mean_vs_truth_with_uncertainties(x, y, x_eval, y_eval, aleatoric_eval, epistemic_eval, ax)\n",
    "\n",
    "#     fig.suptitle(\"Dropout - Learning Rate %f, Epochs %d, Dropout %.3f, Passes %d\" %\n",
    "#                  (learning_rate, epochs, dropout, n_passes))\n",
    "\n",
    "#     # ax.fill_between(x_eval.flatten(), 0, epistemic_eval, label=\"epistemic\", color=\"green\", alpha=0.4)\n",
    "#     # ax.fill_between(x_eval.flatten(), 0, aleatoric_eval, label=\"aleatoric\", color=\"orange\", alpha=0.4)\n",
    "#     ax.legend()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     dropout_values = [0.1, 0.2, 0.3, 0.5]\n",
    "#     fig, axs = plt.subplots(len(dropout_values), 1, figsize=(30, 5*len(dropout_values)), sharey=True)\n",
    "#     axs[0].set_ylim([-1, 3])\n",
    "#     fig.suptitle('Combined-Model | Epochs: 15000, Learning Rate: 1e-3', fontsize=20)\n",
    "#     x, y = sample_generators.generate_osband_sin_samples(60)\n",
    "#     for dropout, ax in zip(dropout_values, axs):\n",
    "#         ax.set_title(\"%.3f Dropout\" % dropout)\n",
    "#         combined_evaluation(x, y, dropout, 1e-3, 20000, 500, ax)\n",
    "#         fig.savefig(\"Combined_Sinus.pdf\")\n",
    "\n",
    "#     fig, axs = plt.subplots(len(dropout_values), 1, figsize=(30, 5*len(dropout_values)), sharey=True)\n",
    "#     fig.suptitle('Combined-Model | Epochs: 15000, Learning Rate: 1e-3', fontsize=20)\n",
    "#     x, y = sample_generators.generate_osband_nonlinear_samples()\n",
    "#     for dropout, ax in zip(dropout_values, axs):\n",
    "#         ax.set_title(\"%.3f Dropout\" % dropout)\n",
    "#         combined_evaluation(x, y, dropout, 1e-3, 20000, 500, ax)\n",
    "#         fig.savefig(\"Combined_Nonlinear.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_truth = data_vector[0:1000,6]\n",
    "y_truth = data_vector[0:1000:,14]\n",
    "dropout = 0.3\n",
    "learning_rate = 1e-4\n",
    "epochs = 5000\n",
    "display_step = 2000\n",
    "\n",
    "combined_training(x_truth, y_truth, dropout, learning_rate, epochs, display_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try creating NN with MCD in tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape before to 3dims, or flatten after first layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data_vector[:,0:14], data_vector[:,14]\n",
    "X_val, y_val = validation_data[:,0:14], validation_data[:,14]\n",
    "\n",
    "# # Convert labels to one-hot encoding\n",
    "# y_train = tf.keras.utils.to_categorical(y_train)\n",
    "# y_val = tf.keras.utils.to_categorical(y_val)\n",
    "\n",
    "# data_vector_expand = np.expand_dims(data_vector, axis=-1)\n",
    "# validation_data_expand = np.expand_dims(validation_data, axis=-1)\n",
    "\n",
    "# X_train, y_train = data_vector_expand[:,0:14], data_vector_expand[:,14]\n",
    "# X_val, y_val = validation_data_expand[:,0:14], validation_data_expand[:,14]\n",
    "\n",
    "pctl = 20\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "input_dims = X_train.shape[1]\n",
    "dropout_rate = 0.3\n",
    "mc_passes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_10 (Lambda)           (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 28)                420       \n",
      "_________________________________________________________________\n",
      "lambda_11 (Lambda)           (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 12)                348       \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 2)                 26        \n",
      "=================================================================\n",
      "Total params: 794\n",
      "Trainable params: 794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 316099 samples, validate on 31609 samples\n",
      "Epoch 1/5\n",
      "316099/316099 [==============================] - 15s 47us/sample - loss: 0.1202 - acc: 0.9579 - val_loss: 0.0219 - val_acc: 0.9970\n",
      "Epoch 2/5\n",
      "316099/316099 [==============================] - 13s 42us/sample - loss: 0.0936 - acc: 0.9652 - val_loss: 0.0194 - val_acc: 0.9970\n",
      "Epoch 3/5\n",
      "316099/316099 [==============================] - 13s 42us/sample - loss: 0.0900 - acc: 0.9662 - val_loss: 0.0196 - val_acc: 0.9966\n",
      "Epoch 4/5\n",
      "316099/316099 [==============================] - 13s 42us/sample - loss: 0.0876 - acc: 0.9669 - val_loss: 0.0169 - val_acc: 0.9965\n",
      "Epoch 5/5\n",
      "316099/316099 [==============================] - 13s 42us/sample - loss: 0.0866 - acc: 0.9673 - val_loss: 0.0179 - val_acc: 0.9966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ecd5f41c88>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_NN_MCD():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(input_dims,))),\n",
    "    model.add(tf.keras.layers.Lambda(lambda x: K.dropout(x, level=0.3))),\n",
    "    model.add(tf.keras.layers.Dense(input_shape=(input_dims,),\n",
    "                                    units=28,\n",
    "                                    activation='relu')),\n",
    "    model.add(tf.keras.layers.Lambda(lambda x: K.dropout(x, level=0.3))),\n",
    "    model.add(tf.keras.layers.Dense(units=12,\n",
    "                                    activation='relu')),\n",
    "#     model.add(tf.keras.layers.Dropout(rate=dropout_rate)(training=True)),\n",
    "#     model.add(tf.keras.layers.Flatten()),\n",
    "    model.add(tf.keras.layers.Dense(2,\n",
    "                                    activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',      \n",
    "#                   metrics=[tf.keras.metrics.Recall()])\n",
    "#                   metrics=[tf.keras.metrics.F1()])\n",
    "                  metrics=['accuracy'])                  \n",
    "    return model\n",
    "\n",
    "NN_MCD = get_NN_MCD()\n",
    "NN_MCD.summary()\n",
    "\n",
    "NN_MCD.fit(X_train, y_train,\n",
    "          batch_size = batch_size,\n",
    "          epochs= epochs,\n",
    "          verbose = 1,\n",
    "          validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctl = [20]\n",
    "data_vector_gaps, data_ind_gaps = preprocessing_gaps(path, img, pctl)\n",
    "X_test, y_test = data_vector_gaps[:,0:14], data_vector_gaps[:,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_passes = 20\n",
    "def predict_with_uncertainty(model, X):\n",
    "    preds = []\n",
    "    for i in range(mc_passes):\n",
    "        preds.append(model.predict(X))\n",
    "    preds = np.array(preds)\n",
    "    means = np.mean(preds, axis=0)\n",
    "    variances = np.var(preds, axis=0)\n",
    "    stds = np.std(preds, axis=0)\n",
    "    pred = np.argmax(means, axis=1)\n",
    "    \n",
    "    return pred, preds, means, variances, stds\n",
    "\n",
    "pred, preds, means, variances, stds = predict_with_uncertainty(NN_MCD, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting how many MC samples predicted flooding?\n",
    "# (# times the prob of flooding > 0.5) / # samples\n",
    "# The color bar should be more saturated in the middle to highlight greater\n",
    "# uncertainty, and lighter towards 0% and 100%.\n",
    "\n",
    "flood = 0\n",
    "flood += "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
