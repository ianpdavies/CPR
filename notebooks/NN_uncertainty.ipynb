{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pyplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7d47d5a68ab6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pyplot'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import math\n",
    "import time\n",
    "from zipfile import *\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.0.0-rc0\n",
      "Python Version: 3.5.5 | packaged by conda-forge | (default, Jul 24 2018, 01:52:17) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "print('Tensorflow version:', tf.__version__)\n",
    "import sys\n",
    "sys.executable\n",
    "sys.path\n",
    "print('Python Version:', sys.version) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from CPR.configs import data_path\n",
    "from CPR.utils import tif_stacker, cloud_generator, preprocessing, train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(CPR.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"stack.tif\" already exists for 4115_LC08_021033_20131227_test\n",
      "Cloud image already exists for 4115_LC08_021033_20131227_test\n"
     ]
    }
   ],
   "source": [
    "# Order in which features should be stacked to create stacked tif\n",
    "feat_list_new = ['aspect','curve', 'developed', 'GSW_distExtent', 'elevation', 'forest',\n",
    " 'GSW_maxExtent', 'hand', 'other_landcover', 'planted', 'slope', 'spi', 'twi', 'wetlands', 'flooded']\n",
    "\n",
    "img_list = ['4115_LC08_021033_20131227_test']\n",
    "\n",
    "for j, img in enumerate(img_list):\n",
    "    #  Stack all the flood imagery\n",
    "    tif_stacker(data_path, img, feat_list_new, overwrite=False)\n",
    "    cloud_generator(img, data_path, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctl = [50]\n",
    "data_train, data_vector_train, data_ind_train = preprocessing(data_path, img, pctl, gaps=False)\n",
    "training_data, validation_data = train_val(data_vector_train, holdout=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NN with MC Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = training_data[:,0:14], training_data[:,14]\n",
    "X_val, y_val = validation_data[:,0:14], validation_data[:,14]\n",
    "\n",
    "pctl = 50\n",
    "INPUT_DIMS = X_train.shape[1:]\n",
    "INPUT_SIZE = X_train.shape[0]\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 100\n",
    "DROPOUT_RATE = 0.3\n",
    "HOLDOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                360       \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 26        \n",
      "=================================================================\n",
      "Total params: 686\n",
      "Trainable params: 686\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 138510 samples, validate on 59361 samples\n",
      "Epoch 1/100\n",
      "138510/138510 [==============================] - 2s 17us/sample - loss: 0.2203 - sparse_categorical_accuracy: 0.9530 - val_loss: 0.0412 - val_sparse_categorical_accuracy: 0.9974\n",
      "Epoch 2/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.1203 - sparse_categorical_accuracy: 0.9616 - val_loss: 0.0273 - val_sparse_categorical_accuracy: 0.9975\n",
      "Epoch 3/100\n",
      "138510/138510 [==============================] - 1s 6us/sample - loss: 0.1078 - sparse_categorical_accuracy: 0.9637 - val_loss: 0.0235 - val_sparse_categorical_accuracy: 0.9974\n",
      "Epoch 4/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.1022 - sparse_categorical_accuracy: 0.9646 - val_loss: 0.0218 - val_sparse_categorical_accuracy: 0.9973\n",
      "Epoch 5/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0990 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.0212 - val_sparse_categorical_accuracy: 0.9972\n",
      "Epoch 6/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0967 - sparse_categorical_accuracy: 0.9648 - val_loss: 0.0200 - val_sparse_categorical_accuracy: 0.9975\n",
      "Epoch 7/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0946 - sparse_categorical_accuracy: 0.9651 - val_loss: 0.0202 - val_sparse_categorical_accuracy: 0.9974\n",
      "Epoch 8/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0935 - sparse_categorical_accuracy: 0.9654 - val_loss: 0.0197 - val_sparse_categorical_accuracy: 0.9973\n",
      "Epoch 9/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0925 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.0182 - val_sparse_categorical_accuracy: 0.9975\n",
      "Epoch 10/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0908 - sparse_categorical_accuracy: 0.9657 - val_loss: 0.0178 - val_sparse_categorical_accuracy: 0.9976\n",
      "Epoch 11/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0896 - sparse_categorical_accuracy: 0.9652 - val_loss: 0.0180 - val_sparse_categorical_accuracy: 0.9975\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21cc7208860>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Functional API\n",
    "# def get_NN_MCD():\n",
    "#     inputs = tf.keras.layers.Input(shape=(INPUT_DIMS))\n",
    "#     x = tf.keras.layers.Dropout(rate=DROPOUT_RATE)(inputs, training=True)\n",
    "#     x = tf.keras.layers.Dense(units=29, activation='relu')(x)\n",
    "#     x = tf.keras.layers.Dropout(rate=DROPOUT_RATE)(x, training=True)  \n",
    "#     x = tf.keras.layers.Dense(units=15, activation='relu')(x)\n",
    "#     outputs = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
    "#     model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "#     model.compile(optimizer='adam',\n",
    "#                   loss='sparse_categorical_crossentropy',      \n",
    "# #                   metrics=[tf.keras.metrics.Recall()])\n",
    "# #                   metrics=[tf.keras.metrics.F1()])\n",
    "#                   metrics=['sparse_categorical_accuracy'])\n",
    "#     return model\n",
    "\n",
    "# Using Sequential API\n",
    "\n",
    "# Custom metric functions\n",
    "\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "def get_NN_MCD():\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=INPUT_DIMS)),\n",
    "    model.add(tf.keras.layers.Lambda(lambda x: K.dropout(x, level=DROPOUT_RATE))),\n",
    "    model.add(tf.keras.layers.Dense(units=24,\n",
    "                                    activation='relu')),\n",
    "    model.add(tf.keras.layers.Lambda(lambda x: K.dropout(x, level=DROPOUT_RATE))),\n",
    "    model.add(tf.keras.layers.Dense(units=12,\n",
    "                                    activation='relu')),\n",
    "#     model.add(tf.keras.layers.Dropout(rate=dropout_rate)(training=True)),\n",
    "#     model.add(tf.keras.layers.Flatten()),\n",
    "    model.add(tf.keras.layers.Dense(2,\n",
    "                                    activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',      \n",
    "#                   metrics=[tf.keras.metrics.Recall()])\n",
    "#                   metrics=[tf.keras.metrics.F1()])\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "# Can maybe use callbacks to get output? https://keras.io/callbacks/\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1, \n",
    "                                      patience=10, verbose=1)\n",
    "\n",
    "# tb = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1,\n",
    "#                                     write_graph=True, batch_size= 2000, write_images=True)\n",
    "\n",
    "NN_MCD = get_NN_MCD()\n",
    "NN_MCD.summary()\n",
    "NN_MCD.fit(X_train, y_train,\n",
    "           batch_size = BATCH_SIZE,\n",
    "           epochs= EPOCHS,\n",
    "           verbose = 1,\n",
    "           validation_data = (X_val, y_val),\n",
    "           callbacks = [es],\n",
    "           use_multiprocessing = True)\n",
    "# model_path = data_path / 'models' / 'cnn_vary_clouds' / img / '{0}'.format(img+'_clouds_'+str(pctl)+'.h5')\n",
    "# NN_MCD.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing NN on cloud gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctl = [20]\n",
    "data_gaps, data_vector_gaps, data_ind_gaps = preprocessing(data_path, img, pctl, gaps=True)\n",
    "\n",
    "X_test, y_test = data_vector_gaps[:,0:14], data_vector_gaps[:,14]\n",
    "\n",
    "def predict_with_uncertainty(model, X):\n",
    "    preds = []\n",
    "    for i in range(mc_passes):\n",
    "        preds.append(model.predict(X))\n",
    "    preds = np.array(preds)\n",
    "    means = np.mean(preds, axis=0)\n",
    "    variances = np.var(preds, axis=0)\n",
    "    stds = np.std(preds, axis=0)\n",
    "    pred = np.argmax(means, axis=1)\n",
    "    \n",
    "#     return pred, preds, means, variances, stds\n",
    "    return pred, variances\n",
    "\n",
    "mc_passes = 100\n",
    "# pred, preds, means, variances, stds = predict_with_uncertainty(NN_MCD, X_test)\n",
    "pred, variances = predict_with_uncertainty(NN_MCD, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting how many MC samples predicted flooding?\n",
    "# (# times the prob of flooding > 0.5) / # samples\n",
    "# The color bar should be more saturated in the middle to highlight greater\n",
    "# uncertainty, and lighter towards 0% and 100%.\n",
    "\n",
    "# flood = 0\n",
    "# flood += "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape gaps back into image\n",
    "stack_path = data_path / 'images' / img / 'stack' / 'stack.tif'\n",
    "with rasterio.open(str(stack_path), 'r') as ds:\n",
    "        shape = ds.read(1).shape # Shape of full original image\n",
    "        arr_empty = np.zeros(shape) # Create empty array with this shape\n",
    "        arr_empty[:] = np.nan # Convert all zeroes to NaN\n",
    "        var_img = arr_empty\n",
    "        rows, cols = zip(data_ind_gaps)\n",
    "        var_img[rows, cols] = variances[:,0]\n",
    "plt.imshow(var_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NN on multiple images and cloud covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NN_MCD():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=INPUT_DIMS)),\n",
    "    model.add(tf.keras.layers.Lambda(lambda x: K.dropout(x, level=DROPOUT_RATE))),\n",
    "    model.add(tf.keras.layers.Dense(units=29,\n",
    "                                    activation='relu')),\n",
    "    model.add(tf.keras.layers.Lambda(lambda x: K.dropout(x, level=DROPOUT_RATE))),\n",
    "    model.add(tf.keras.layers.Dense(units=15,\n",
    "                                    activation='relu')),\n",
    "#     model.add(tf.keras.layers.Dropout(rate=dropout_rate)(training=True)),\n",
    "#     model.add(tf.keras.layers.Flatten()),\n",
    "    model.add(tf.keras.layers.Dense(2,\n",
    "                                    activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',      \n",
    "#                   metrics=[tf.keras.metrics.Recall()])\n",
    "#                   metrics=[tf.keras.metrics.F1()])\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from CPR.configs import data_path\n",
    "from CPR.utils import tif_stacker, cloud_generator, preprocessing, train_val, timer\n",
    "\n",
    "# Order in which features should be stacked to create stacked tif\n",
    "feat_list_new = ['aspect','curve', 'developed', 'GSW_distExtent', 'elevation', 'forest',\n",
    " 'GSW_maxExtent', 'hand', 'other_landcover', 'planted', 'slope', 'spi', 'twi', 'wetlands', 'flooded']\n",
    "\n",
    "# Image to predict on\n",
    "# img_list = ['4115_LC08_021033_20131227_test']\n",
    "img_list = ['4101_LC08_027038_20131103_1']\n",
    "#             '4101_LC08_027038_20131103_2' \n",
    "#             '4101_LC08_027039_20131103_1',\n",
    "#             '4115_LC08_021033_20131227_1',\n",
    "#             '4337_LC08_026038_20160325_1']\n",
    "\n",
    "pctls = [10,20,30,40,50,60,70,80,90]\n",
    "# pctls = [40, 50]\n",
    "batch_size = 7000\n",
    "epochs = 100\n",
    "dropout_rate = 0.3\n",
    "holdout = 0.3 # Validation data size\n",
    "cb_list = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                 min_delta=0.5, \n",
    "                                                 patience=10, \n",
    "                                                 verbose=1)]\n",
    "valMetricsList = []\n",
    "\n",
    "# Stack layers into a single tif, and generate cloud cover image\n",
    "for j, img in enumerate(img_list):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    accuracy = []\n",
    "    times = []\n",
    "    history = []\n",
    "    \n",
    "    tif_stacker(data_path, img, feat_list_new, overwrite=False)\n",
    "    cloud_generator(img, data_path, overwrite=False)\n",
    "    \n",
    "    for i, pctl in enumerate(pctls):\n",
    "        data_train, data_vector_train, data_ind_train = preprocessing(data_path, img, pctl, gaps=False)\n",
    "        training_data, validation_data = train_val(data_vector_train, holdout=holdout)\n",
    "        X_train, y_train = training_data[:,0:14], training_data[:,14]\n",
    "        X_val, y_val = validation_data[:,0:14], validation_data[:,14]\n",
    "        INPUT_DIMS = X_train.shape[1]\n",
    "                           \n",
    "        print('~~~~~', img, pctl+'% cloud cover')\n",
    "        \n",
    "        NN_MCD = get_NN_MCD()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        NN_MCD.fit(X_train, y_train,\n",
    "                   batch_size = batch_size,\n",
    "                   epochs= epochs,\n",
    "                   verbose = 2,\n",
    "                   validation_data = (X_val, y_val),\n",
    "                   callbacks=cb_list,\n",
    "                   use_multiprocessing = True)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        times.append(timer(start_time, end_time, False))\n",
    "\n",
    "        model_path = data_path / 'models' / 'cnn_vary_clouds' / img / '{0}'.format(img+'_clouds_'+str(pctl)+'.h5')\n",
    "        NN_MCD.save(model_path)\n",
    "    \n",
    "    metrics_path = data_path / 'metrics' / 'testing' / img \n",
    "    \n",
    "    try:\n",
    "        metrics_path.mkdir(parents=True)\n",
    "    except FileExistsError:\n",
    "        print('Metrics directory already exists')\n",
    "    \n",
    "    times = [float(i) for i in times]\n",
    "    times_df = pd.DataFrame(times, columns = ['times'])\n",
    "    times_df.to_csv(metrics_path / 'training_times.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a problem loading keras models: https://github.com/keras-team/keras/issues/10417\n",
    "# But loading the weights into an identical compiled model works\n",
    "\n",
    "import pickle\n",
    "\n",
    "def predict_with_uncertainty(model, X):\n",
    "    preds = []\n",
    "    for i in range(MC_PASSES):\n",
    "        if i % 10 == 0 or i == MC_PASSES - 1:\n",
    "            print('Running MC '+str(i)+'/'+str(MC_PASSES))\n",
    "        preds.append(model.predict(X, batch_size=7000, use_multiprocessing=True))\n",
    "    preds = np.array(preds)\n",
    "    means = np.mean(preds, axis=0)\n",
    "    variances = np.var(preds, axis=0)\n",
    "    stds = np.std(preds, axis=0)\n",
    "    pred = np.argmax(means, axis=1)\n",
    "    pred = list(pred)\n",
    "#     return pred, preds, means, variances, stds\n",
    "    return pred, variances\n",
    "\n",
    "MC_PASSES = 100\n",
    "\n",
    "for j, img in enumerate(img_list):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    accuracy = []\n",
    "    times = []\n",
    "    predictions = []\n",
    "    gapMetricsList = []\n",
    "    variances = []\n",
    "    \n",
    "    for i, pctl in enumerate(pctls):\n",
    "        data_test, data_vector_test, data_ind_test = preprocessing(data_path, img, pctl, gaps=True)\n",
    "        X_test, y_test = data_vector_test[:,0:14], data_vector_test[:,14]\n",
    "        \n",
    "        print(img, pctl)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        NN_MCD = get_NN_MCD() # Get untrained model to add trained weights into\n",
    "        model_path = data_path / 'models' / 'cnn_vary_clouds' / img / '{0}'.format(img+'_clouds_'+str(pctl)+'.h5')\n",
    "        NN_MCD.load_weights(str(model_path))\n",
    "        preds, variances = predict_with_uncertainty(NN_MCD, X_test)\n",
    "        \n",
    "        times.append(timer(start_time, time.time(), False)) # Elapsed time for MC simulations\n",
    "        predictions.append(list(preds))\n",
    "        accuracy.append(sklearn.metrics.accuracy_score(y_test, preds))\n",
    "        precision.append(sklearn.metrics.precision_score(y_test, preds))\n",
    "        recall.append(sklearn.metrics.recall_score(y_test, preds))\n",
    "        f1.append(sklearn.metrics.f1_score(y_test, preds))\n",
    "    \n",
    "    metrics_path = data_path / 'metrics' / 'testing' / img \n",
    "    \n",
    "    try:\n",
    "        metrics_path.mkdir(parents=True)\n",
    "    except FileExistsError:\n",
    "        print('Metrics directory already exists')\n",
    "        \n",
    "    with open(str(metrics_path / 'predictions.pkl'), 'wb') as outfile:\n",
    "        pickle.dump(predictions, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    times = [float(i) for i in times] # Need to convert time objects to float, otherwise valMetrics will be non-numeric\n",
    "        \n",
    "    gapMetrics = pd.DataFrame(np.column_stack([pctls, accuracy, precision, recall, f1, times]),\n",
    "                          columns=['cloud_cover','accuracy','precision','recall','f1', 'time'])\n",
    "    \n",
    "    gapMetrics.to_csv(metrics_path / 'gapMetrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look into using GPU to improve speed\n",
    "# Also callbacks like EarlyStopping to halt training once performance stops improving"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
