{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import math\n",
    "import time\n",
    "from zipfile import *\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.0.0-rc1\n",
      "Python Version: 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "print('Tensorflow version:', tf.__version__)\n",
    "import sys\n",
    "sys.executable\n",
    "sys.path\n",
    "print('Python Version:', sys.version) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from CPR.configs import data_path\n",
    "from CPR.utils import tif_stacker, cloud_generator, preprocessing, train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(CPR.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"stack.tif\" already exists for 4115_LC08_021033_20131227_test\n",
      "Cloud image already exists for 4115_LC08_021033_20131227_test\n"
     ]
    }
   ],
   "source": [
    "# Order in which features should be stacked to create stacked tif\n",
    "feat_list_new = ['aspect','curve', 'developed', 'GSW_distExtent', 'elevation', 'forest',\n",
    " 'GSW_maxExtent', 'hand', 'other_landcover', 'planted', 'slope', 'spi', 'twi', 'wetlands', 'flooded']\n",
    "\n",
    "img_list = ['4115_LC08_021033_20131227_test']\n",
    "\n",
    "for j, img in enumerate(img_list):\n",
    "    #  Stack all the flood imagery\n",
    "    tif_stacker(data_path, img, feat_list_new, overwrite=False)\n",
    "    cloud_generator(img, data_path, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctl = [50]\n",
    "data_train, data_vector_train, data_ind_train = preprocessing(data_path, img, pctl, gaps=False)\n",
    "training_data, validation_data = train_val(data_vector_train, holdout=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NN with MC Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = training_data[:,0:14], training_data[:,14]\n",
    "X_val, y_val = validation_data[:,0:14], validation_data[:,14]\n",
    "\n",
    "pctl = 50\n",
    "INPUT_DIMS = X_train.shape[1:]\n",
    "INPUT_SIZE = X_train.shape[0]\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 100\n",
    "DROPOUT_RATE = 0.3\n",
    "HOLDOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x145d3367748>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_MCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                360       \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 26        \n",
      "=================================================================\n",
      "Total params: 686\n",
      "Trainable params: 686\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 138510 samples, validate on 59361 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000145D4187730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000145D4187730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "138510/138510 [==============================] - 2s 13us/sample - loss: 0.3487 - sparse_categorical_accuracy: 0.8738 - val_loss: 0.0873 - val_sparse_categorical_accuracy: 0.9973\n",
      "Epoch 2/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.1376 - sparse_categorical_accuracy: 0.9609 - val_loss: 0.0422 - val_sparse_categorical_accuracy: 0.9971\n",
      "Epoch 3/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.1184 - sparse_categorical_accuracy: 0.9628 - val_loss: 0.0322 - val_sparse_categorical_accuracy: 0.9972\n",
      "Epoch 4/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.1086 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.0278 - val_sparse_categorical_accuracy: 0.9973\n",
      "Epoch 5/100\n",
      "138510/138510 [==============================] - 1s 8us/sample - loss: 0.1040 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.0243 - val_sparse_categorical_accuracy: 0.9972\n",
      "Epoch 6/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.1022 - sparse_categorical_accuracy: 0.9653 - val_loss: 0.0228 - val_sparse_categorical_accuracy: 0.9976\n",
      "Epoch 7/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.1007 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.0222 - val_sparse_categorical_accuracy: 0.9974\n",
      "Epoch 8/100\n",
      "138510/138510 [==============================] - 1s 8us/sample - loss: 0.0980 - sparse_categorical_accuracy: 0.9653 - val_loss: 0.0218 - val_sparse_categorical_accuracy: 0.9974\n",
      "Epoch 9/100\n",
      "138510/138510 [==============================] - 1s 8us/sample - loss: 0.0969 - sparse_categorical_accuracy: 0.9656 - val_loss: 0.0206 - val_sparse_categorical_accuracy: 0.9975\n",
      "Epoch 10/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0951 - sparse_categorical_accuracy: 0.9658 - val_loss: 0.0205 - val_sparse_categorical_accuracy: 0.9975\n",
      "Epoch 11/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0915 - sparse_categorical_accuracy: 0.9664 - val_loss: 0.0193 - val_sparse_categorical_accuracy: 0.9976\n",
      "Epoch 12/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0926 - sparse_categorical_accuracy: 0.9662 - val_loss: 0.0180 - val_sparse_categorical_accuracy: 0.9977\n",
      "Epoch 13/100\n",
      "138510/138510 [==============================] - 1s 9us/sample - loss: 0.0924 - sparse_categorical_accuracy: 0.9659 - val_loss: 0.0195 - val_sparse_categorical_accuracy: 0.9975\n",
      "Epoch 14/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0911 - sparse_categorical_accuracy: 0.9660 - val_loss: 0.0195 - val_sparse_categorical_accuracy: 0.9974\n",
      "Epoch 15/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0899 - sparse_categorical_accuracy: 0.9662 - val_loss: 0.0184 - val_sparse_categorical_accuracy: 0.9975\n",
      "Epoch 16/100\n",
      "138510/138510 [==============================] - 1s 8us/sample - loss: 0.0901 - sparse_categorical_accuracy: 0.9661 - val_loss: 0.0188 - val_sparse_categorical_accuracy: 0.9975\n",
      "Epoch 17/100\n",
      "138510/138510 [==============================] - 1s 8us/sample - loss: 0.0883 - sparse_categorical_accuracy: 0.9663 - val_loss: 0.0180 - val_sparse_categorical_accuracy: 0.9976\n",
      "Epoch 18/100\n",
      "138510/138510 [==============================] - 1s 8us/sample - loss: 0.0866 - sparse_categorical_accuracy: 0.9673 - val_loss: 0.0166 - val_sparse_categorical_accuracy: 0.9978\n",
      "Epoch 19/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0876 - sparse_categorical_accuracy: 0.9667 - val_loss: 0.0180 - val_sparse_categorical_accuracy: 0.9977\n",
      "Epoch 20/100\n",
      "138510/138510 [==============================] - 1s 7us/sample - loss: 0.0868 - sparse_categorical_accuracy: 0.9667 - val_loss: 0.0179 - val_sparse_categorical_accuracy: 0.9977\n",
      "Epoch 21/100\n",
      "138510/138510 [==============================] - 1s 8us/sample - loss: 0.0869 - sparse_categorical_accuracy: 0.9669 - val_loss: 0.0173 - val_sparse_categorical_accuracy: 0.9977\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x145d3e3eb70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_NN_MCD():\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=INPUT_DIMS)),\n",
    "    model.add(tf.keras.layers.Lambda(lambda x: K.dropout(x, level=DROPOUT_RATE))),\n",
    "    model.add(tf.keras.layers.Dense(units=24,\n",
    "                                    activation='relu')),\n",
    "    model.add(tf.keras.layers.Lambda(lambda x: K.dropout(x, level=DROPOUT_RATE))),\n",
    "    model.add(tf.keras.layers.Dense(units=12,\n",
    "                                    activation='relu')),\n",
    "#     model.add(tf.keras.layers.Dropout(rate=dropout_rate)(training=True)),\n",
    "#     model.add(tf.keras.layers.Flatten()),\n",
    "    model.add(tf.keras.layers.Dense(2,\n",
    "                                    activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',      \n",
    "#                   metrics=[tf.keras.metrics.Recall()])\n",
    "#                   metrics=[tf.keras.metrics.F1()])\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "# es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.005, verbose=1, mode='auto')\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.005, verbose=1, patience=10, mode='auto')\n",
    "\n",
    "# tb = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1,\n",
    "#                                     write_graph=True, batch_size= 2000, write_images=True)\n",
    "\n",
    "NN_MCD = get_NN_MCD()\n",
    "NN_MCD.summary()\n",
    "NN_MCD.fit(X_train, y_train,\n",
    "           batch_size = BATCH_SIZE,\n",
    "           epochs= EPOCHS,\n",
    "           verbose = 1,\n",
    "           validation_data = (X_val, y_val),\n",
    "           callbacks = [es],\n",
    "           use_multiprocessing = True)\n",
    "# model_path = data_path / 'models' / 'cnn_vary_clouds' / img / '{0}'.format(img+'_clouds_'+str(pctl)+'.h5')\n",
    "# NN_MCD.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing NN on cloud gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (559,720) (9,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f5707f06e0dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpctl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_gaps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_vector_gaps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_ind_gaps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpctl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgaps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_vector_gaps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_vector_gaps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\CPR\\CPR\\utils.py\u001b[0m in \u001b[0;36mpreprocessing\u001b[1;34m(data_path, img, pctl, gaps)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgaps\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mcloudMask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcloudMask\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloudMask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpctl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;31m# Need to remove NaNs because any arithmetic operation involving an NaN will result in NaN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (559,720) (9,) "
     ]
    }
   ],
   "source": [
    "pctl = [20]\n",
    "data_gaps, data_vector_gaps, data_ind_gaps = preprocessing(data_path, img, pctl, gaps=True)\n",
    "\n",
    "X_test, y_test = data_vector_gaps[:,0:14], data_vector_gaps[:,14]\n",
    "\n",
    "def predict_with_uncertainty(model, X):\n",
    "    preds = []\n",
    "    for i in range(mc_passes):\n",
    "        preds.append(model.predict(X))\n",
    "    preds = np.array(preds)\n",
    "    means = np.mean(preds, axis=0)\n",
    "    variances = np.var(preds, axis=0)\n",
    "    stds = np.std(preds, axis=0)\n",
    "    pred = np.argmax(means, axis=1)\n",
    "    \n",
    "#     return pred, preds, means, variances, stds\n",
    "    return pred, variances\n",
    "\n",
    "mc_passes = 100\n",
    "# pred, preds, means, variances, stds = predict_with_uncertainty(NN_MCD, X_test)\n",
    "pred, variances = predict_with_uncertainty(NN_MCD, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting how many MC samples predicted flooding?\n",
    "# (# times the prob of flooding > 0.5) / # samples\n",
    "# The color bar should be more saturated in the middle to highlight greater\n",
    "# uncertainty, and lighter towards 0% and 100%.\n",
    "\n",
    "# flood = 0\n",
    "# flood += "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape gaps back into image\n",
    "stack_path = data_path / 'images' / img / 'stack' / 'stack.tif'\n",
    "with rasterio.open(str(stack_path), 'r') as ds:\n",
    "        shape = ds.read(1).shape # Shape of full original image\n",
    "        arr_empty = np.zeros(shape) # Create empty array with this shape\n",
    "        arr_empty[:] = np.nan # Convert all zeroes to NaN\n",
    "        var_img = arr_empty\n",
    "        rows, cols = zip(data_ind_gaps)\n",
    "        var_img[rows, cols] = variances[:,0]\n",
    "plt.imshow(var_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NN on multiple images and cloud covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NN_MCD():\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=INPUT_DIMS)),\n",
    "    model.add(tf.keras.layers.Lambda(lambda x: K.dropout(x, level=DROPOUT_RATE))),\n",
    "    model.add(tf.keras.layers.Dense(units=24,\n",
    "                                    activation='relu')),\n",
    "    model.add(tf.keras.layers.Lambda(lambda x: K.dropout(x, level=DROPOUT_RATE))),\n",
    "    model.add(tf.keras.layers.Dense(units=12,\n",
    "                                    activation='relu')),\n",
    "#     model.add(tf.keras.layers.Dropout(rate=dropout_rate)(training=True)),\n",
    "#     model.add(tf.keras.layers.Flatten()),\n",
    "    model.add(tf.keras.layers.Dense(2,\n",
    "                                    activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from CPR.configs import data_path\n",
    "from CPR.utils import tif_stacker, cloud_generator, preprocessing, train_val, timer\n",
    "\n",
    "# Order in which features should be stacked to create stacked tif\n",
    "feat_list_new = ['aspect','curve', 'developed', 'GSW_distExtent', 'elevation', 'forest',\n",
    " 'GSW_maxExtent', 'hand', 'other_landcover', 'planted', 'slope', 'spi', 'twi', 'wetlands', 'flooded']\n",
    "\n",
    "# Image to predict on\n",
    "# img_list = ['4115_LC08_021033_20131227_test']\n",
    "img_list = ['4101_LC08_027038_20131103_1']\n",
    "#             '4101_LC08_027038_20131103_2' \n",
    "#             '4101_LC08_027039_20131103_1',\n",
    "#             '4115_LC08_021033_20131227_1',\n",
    "#             '4337_LC08_026038_20160325_1']\n",
    "\n",
    "pctls = [10,20,30,40,50,60,70,80,90]\n",
    "# pctls = [40, 50]\n",
    "batch_size = 7000\n",
    "epochs = 1000\n",
    "dropout_rate = 0.3\n",
    "holdout = 0.3 # Validation data size\n",
    "cb_list = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                 min_delta=0.5, \n",
    "                                                 patience=10, \n",
    "                                                 verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"stack.tif\" already exists for 4101_LC08_027038_20131103_1\n",
      "Cloud image already exists for 4101_LC08_027038_20131103_1\n",
      "~~~~~ 4101_LC08_027038_20131103_1 10 % cloud cover\n",
      "Train on 8006443 samples, validate on 3431333 samples\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000259242FBE18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000259242FBE18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "8006443/8006443 [==============================] - 19s 2us/sample - loss: 0.0991 - sparse_categorical_accuracy: 0.9645 - val_loss: 0.0378 - val_sparse_categorical_accuracy: 0.9962\n",
      "Epoch 2/1000\n",
      "8006443/8006443 [==============================] - 16s 2us/sample - loss: 0.0440 - sparse_categorical_accuracy: 0.9869 - val_loss: 0.0497 - val_sparse_categorical_accuracy: 0.9909\n",
      "Epoch 3/1000\n",
      "8006443/8006443 [==============================] - 16s 2us/sample - loss: 0.0398 - sparse_categorical_accuracy: 0.9881 - val_loss: 0.0515 - val_sparse_categorical_accuracy: 0.9880\n",
      "Epoch 4/1000\n",
      "8006443/8006443 [==============================] - 16s 2us/sample - loss: 0.0379 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0627 - val_sparse_categorical_accuracy: 0.9843\n",
      "Epoch 5/1000\n",
      "8006443/8006443 [==============================] - 16s 2us/sample - loss: 0.0367 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.0732 - val_sparse_categorical_accuracy: 0.9818\n",
      "Epoch 6/1000\n",
      "8006443/8006443 [==============================] - 16s 2us/sample - loss: 0.0356 - sparse_categorical_accuracy: 0.9892 - val_loss: 0.0913 - val_sparse_categorical_accuracy: 0.9690\n",
      "Epoch 7/1000\n",
      "8006443/8006443 [==============================] - 16s 2us/sample - loss: 0.0350 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.1086 - val_sparse_categorical_accuracy: 0.9573\n",
      "Epoch 8/1000\n",
      "8006443/8006443 [==============================] - 16s 2us/sample - loss: 0.0342 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.1030 - val_sparse_categorical_accuracy: 0.9628\n",
      "Epoch 9/1000\n",
      "8006443/8006443 [==============================] - 17s 2us/sample - loss: 0.0338 - sparse_categorical_accuracy: 0.9896 - val_loss: 0.1177 - val_sparse_categorical_accuracy: 0.9534\n",
      "Epoch 10/1000\n",
      "8006443/8006443 [==============================] - 18s 2us/sample - loss: 0.0335 - sparse_categorical_accuracy: 0.9896 - val_loss: 0.1223 - val_sparse_categorical_accuracy: 0.9508\n",
      "Epoch 11/1000\n",
      "8006443/8006443 [==============================] - 14s 2us/sample - loss: 0.0334 - sparse_categorical_accuracy: 0.9896 - val_loss: 0.1282 - val_sparse_categorical_accuracy: 0.9446\n",
      "Epoch 00011: early stopping\n",
      "~~~~~ 4101_LC08_027038_20131103_1 20 % cloud cover\n",
      "Train on 7099955 samples, validate on 3042837 samples\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002592582AEA0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002592582AEA0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "7099955/7099955 [==============================] - 17s 2us/sample - loss: 0.0800 - sparse_categorical_accuracy: 0.9795 - val_loss: 0.0346 - val_sparse_categorical_accuracy: 0.9946\n",
      "Epoch 2/1000\n",
      "7099955/7099955 [==============================] - 14s 2us/sample - loss: 0.0418 - sparse_categorical_accuracy: 0.9878 - val_loss: 0.0403 - val_sparse_categorical_accuracy: 0.9924\n",
      "Epoch 3/1000\n",
      "7099955/7099955 [==============================] - 15s 2us/sample - loss: 0.0382 - sparse_categorical_accuracy: 0.9886 - val_loss: 0.0749 - val_sparse_categorical_accuracy: 0.9711\n",
      "Epoch 4/1000\n",
      "7099955/7099955 [==============================] - 11s 2us/sample - loss: 0.0368 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.1073 - val_sparse_categorical_accuracy: 0.9621\n",
      "Epoch 5/1000\n",
      "7099955/7099955 [==============================] - 9s 1us/sample - loss: 0.0358 - sparse_categorical_accuracy: 0.9891 - val_loss: 0.2852 - val_sparse_categorical_accuracy: 0.9319\n",
      "Epoch 6/1000\n",
      "7099955/7099955 [==============================] - 15s 2us/sample - loss: 0.0353 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.7003 - val_sparse_categorical_accuracy: 0.8906\n",
      "Epoch 7/1000\n",
      "7099955/7099955 [==============================] - 14s 2us/sample - loss: 0.0348 - sparse_categorical_accuracy: 0.9894 - val_loss: 1.1741 - val_sparse_categorical_accuracy: 0.8756\n",
      "Epoch 8/1000\n",
      "7099955/7099955 [==============================] - 14s 2us/sample - loss: 0.0345 - sparse_categorical_accuracy: 0.9894 - val_loss: 1.4335 - val_sparse_categorical_accuracy: 0.8730\n",
      "Epoch 9/1000\n",
      "7099955/7099955 [==============================] - 15s 2us/sample - loss: 0.0342 - sparse_categorical_accuracy: 0.9895 - val_loss: 1.4847 - val_sparse_categorical_accuracy: 0.8727\n",
      "Epoch 10/1000\n",
      "7099955/7099955 [==============================] - 12s 2us/sample - loss: 0.0337 - sparse_categorical_accuracy: 0.9896 - val_loss: 1.5921 - val_sparse_categorical_accuracy: 0.8731\n",
      "Epoch 11/1000\n",
      "7099955/7099955 [==============================] - 13s 2us/sample - loss: 0.0334 - sparse_categorical_accuracy: 0.9896 - val_loss: 1.8059 - val_sparse_categorical_accuracy: 0.8711\n",
      "Epoch 00011: early stopping\n",
      "~~~~~ 4101_LC08_027038_20131103_1 30 % cloud cover\n",
      "Train on 6197194 samples, validate on 2655940 samples\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002592587F268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002592587F268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "6197194/6197194 [==============================] - 14s 2us/sample - loss: 0.0982 - sparse_categorical_accuracy: 0.9675 - val_loss: 0.0375 - val_sparse_categorical_accuracy: 0.9926\n",
      "Epoch 2/1000\n",
      "6197194/6197194 [==============================] - 13s 2us/sample - loss: 0.0414 - sparse_categorical_accuracy: 0.9877 - val_loss: 0.0379 - val_sparse_categorical_accuracy: 0.9920\n",
      "Epoch 3/1000\n",
      "6197194/6197194 [==============================] - 11s 2us/sample - loss: 0.0375 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0433 - val_sparse_categorical_accuracy: 0.9911\n",
      "Epoch 4/1000\n",
      "6197194/6197194 [==============================] - 11s 2us/sample - loss: 0.0357 - sparse_categorical_accuracy: 0.9888 - val_loss: 0.0414 - val_sparse_categorical_accuracy: 0.9909\n",
      "Epoch 5/1000\n",
      "6197194/6197194 [==============================] - 13s 2us/sample - loss: 0.0349 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.0449 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 6/1000\n",
      "6197194/6197194 [==============================] - 12s 2us/sample - loss: 0.0343 - sparse_categorical_accuracy: 0.9891 - val_loss: 0.0415 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 7/1000\n",
      "6197194/6197194 [==============================] - 13s 2us/sample - loss: 0.0336 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0431 - val_sparse_categorical_accuracy: 0.9873\n",
      "Epoch 8/1000\n",
      "6197194/6197194 [==============================] - 12s 2us/sample - loss: 0.0333 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0438 - val_sparse_categorical_accuracy: 0.9865\n",
      "Epoch 9/1000\n",
      "6197194/6197194 [==============================] - 12s 2us/sample - loss: 0.0328 - sparse_categorical_accuracy: 0.9897 - val_loss: 0.0535 - val_sparse_categorical_accuracy: 0.9831\n",
      "Epoch 10/1000\n",
      "6197194/6197194 [==============================] - 12s 2us/sample - loss: 0.0324 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.0642 - val_sparse_categorical_accuracy: 0.9797\n",
      "Epoch 11/1000\n",
      "6197194/6197194 [==============================] - 12s 2us/sample - loss: 0.0321 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.0593 - val_sparse_categorical_accuracy: 0.9818\n",
      "Epoch 00011: early stopping\n",
      "~~~~~ 4101_LC08_027038_20131103_1 40 % cloud cover\n",
      "Train on 5294438 samples, validate on 2269044 samples\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002592772AB70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002592772AB70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "5294438/5294438 [==============================] - 13s 2us/sample - loss: 0.0646 - sparse_categorical_accuracy: 0.9861 - val_loss: 0.0406 - val_sparse_categorical_accuracy: 0.9906\n",
      "Epoch 2/1000\n",
      "5294438/5294438 [==============================] - 12s 2us/sample - loss: 0.0376 - sparse_categorical_accuracy: 0.9885 - val_loss: 0.0419 - val_sparse_categorical_accuracy: 0.9906\n",
      "Epoch 3/1000\n",
      "5294438/5294438 [==============================] - 11s 2us/sample - loss: 0.0344 - sparse_categorical_accuracy: 0.9892 - val_loss: 0.0475 - val_sparse_categorical_accuracy: 0.9889\n",
      "Epoch 4/1000\n",
      "5294438/5294438 [==============================] - 11s 2us/sample - loss: 0.0329 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0599 - val_sparse_categorical_accuracy: 0.9820\n",
      "Epoch 5/1000\n",
      "5294438/5294438 [==============================] - 11s 2us/sample - loss: 0.0321 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.0554 - val_sparse_categorical_accuracy: 0.9827\n",
      "Epoch 6/1000\n",
      "5294438/5294438 [==============================] - 9s 2us/sample - loss: 0.0314 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.0611 - val_sparse_categorical_accuracy: 0.9785\n",
      "Epoch 7/1000\n",
      "5294438/5294438 [==============================] - 9s 2us/sample - loss: 0.0311 - sparse_categorical_accuracy: 0.9901 - val_loss: 0.0760 - val_sparse_categorical_accuracy: 0.9713\n",
      "Epoch 8/1000\n",
      "5294438/5294438 [==============================] - 11s 2us/sample - loss: 0.0309 - sparse_categorical_accuracy: 0.9902 - val_loss: 0.0691 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 9/1000\n",
      "5294438/5294438 [==============================] - 9s 2us/sample - loss: 0.0306 - sparse_categorical_accuracy: 0.9903 - val_loss: 0.0807 - val_sparse_categorical_accuracy: 0.9700\n",
      "Epoch 10/1000\n",
      "5294438/5294438 [==============================] - 11s 2us/sample - loss: 0.0303 - sparse_categorical_accuracy: 0.9904 - val_loss: 0.0767 - val_sparse_categorical_accuracy: 0.9712\n",
      "Epoch 11/1000\n",
      "5294438/5294438 [==============================] - 11s 2us/sample - loss: 0.0303 - sparse_categorical_accuracy: 0.9904 - val_loss: 0.0693 - val_sparse_categorical_accuracy: 0.9760\n",
      "Epoch 00011: early stopping\n",
      "~~~~~ 4101_LC08_027038_20131103_1 50 % cloud cover\n",
      "Train on 4406035 samples, validate on 1888301 samples\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000259242FB1E0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000259242FB1E0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "4406035/4406035 [==============================] - 10s 2us/sample - loss: 0.1248 - sparse_categorical_accuracy: 0.9604 - val_loss: 0.0453 - val_sparse_categorical_accuracy: 0.9881\n",
      "Epoch 2/1000\n",
      "4406035/4406035 [==============================] - 9s 2us/sample - loss: 0.0421 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.0409 - val_sparse_categorical_accuracy: 0.9895\n",
      "Epoch 3/1000\n",
      "4406035/4406035 [==============================] - 9s 2us/sample - loss: 0.0382 - sparse_categorical_accuracy: 0.9888 - val_loss: 0.0397 - val_sparse_categorical_accuracy: 0.9897\n",
      "Epoch 4/1000\n",
      "4406035/4406035 [==============================] - 9s 2us/sample - loss: 0.0360 - sparse_categorical_accuracy: 0.9892 - val_loss: 0.0403 - val_sparse_categorical_accuracy: 0.9898\n",
      "Epoch 5/1000\n",
      "4406035/4406035 [==============================] - 9s 2us/sample - loss: 0.0340 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0424 - val_sparse_categorical_accuracy: 0.9898\n",
      "Epoch 6/1000\n",
      "4406035/4406035 [==============================] - 9s 2us/sample - loss: 0.0326 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.0463 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 7/1000\n",
      "4406035/4406035 [==============================] - 8s 2us/sample - loss: 0.0318 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.0460 - val_sparse_categorical_accuracy: 0.9884\n",
      "Epoch 8/1000\n",
      "4406035/4406035 [==============================] - 7s 2us/sample - loss: 0.0312 - sparse_categorical_accuracy: 0.9902 - val_loss: 0.0467 - val_sparse_categorical_accuracy: 0.9881\n",
      "Epoch 9/1000\n",
      "4406035/4406035 [==============================] - 8s 2us/sample - loss: 0.0309 - sparse_categorical_accuracy: 0.9903 - val_loss: 0.0467 - val_sparse_categorical_accuracy: 0.9879\n",
      "Epoch 10/1000\n",
      "4406035/4406035 [==============================] - 6s 1us/sample - loss: 0.0308 - sparse_categorical_accuracy: 0.9903 - val_loss: 0.0441 - val_sparse_categorical_accuracy: 0.9884\n",
      "Epoch 11/1000\n",
      "4406035/4406035 [==============================] - 5s 1us/sample - loss: 0.0304 - sparse_categorical_accuracy: 0.9904 - val_loss: 0.0454 - val_sparse_categorical_accuracy: 0.9881\n",
      "Epoch 00011: early stopping\n",
      "~~~~~ 4101_LC08_027038_20131103_1 60 % cloud cover\n",
      "Train on 3521240 samples, validate on 1509102 samples\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025924BDCB70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025924BDCB70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "3521240/3521240 [==============================] - 10s 3us/sample - loss: 0.0921 - sparse_categorical_accuracy: 0.9748 - val_loss: 0.0446 - val_sparse_categorical_accuracy: 0.9870\n",
      "Epoch 2/1000\n",
      "3521240/3521240 [==============================] - 6s 2us/sample - loss: 0.0395 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.0410 - val_sparse_categorical_accuracy: 0.9881\n",
      "Epoch 3/1000\n",
      "3521240/3521240 [==============================] - 6s 2us/sample - loss: 0.0360 - sparse_categorical_accuracy: 0.9892 - val_loss: 0.0401 - val_sparse_categorical_accuracy: 0.9887\n",
      "Epoch 4/1000\n",
      "3521240/3521240 [==============================] - 6s 2us/sample - loss: 0.0338 - sparse_categorical_accuracy: 0.9896 - val_loss: 0.0391 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 5/1000\n",
      "3521240/3521240 [==============================] - 6s 2us/sample - loss: 0.0322 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.0383 - val_sparse_categorical_accuracy: 0.9891\n",
      "Epoch 6/1000\n",
      "3521240/3521240 [==============================] - 6s 2us/sample - loss: 0.0315 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.0374 - val_sparse_categorical_accuracy: 0.9893\n",
      "Epoch 7/1000\n",
      "3521240/3521240 [==============================] - 7s 2us/sample - loss: 0.0310 - sparse_categorical_accuracy: 0.9902 - val_loss: 0.0387 - val_sparse_categorical_accuracy: 0.9891\n",
      "Epoch 8/1000\n",
      "3521240/3521240 [==============================] - 7s 2us/sample - loss: 0.0307 - sparse_categorical_accuracy: 0.9903 - val_loss: 0.0393 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 9/1000\n",
      "3521240/3521240 [==============================] - 7s 2us/sample - loss: 0.0303 - sparse_categorical_accuracy: 0.9905 - val_loss: 0.0385 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 10/1000\n",
      "3521240/3521240 [==============================] - 7s 2us/sample - loss: 0.0301 - sparse_categorical_accuracy: 0.9907 - val_loss: 0.0382 - val_sparse_categorical_accuracy: 0.9892\n",
      "Epoch 11/1000\n",
      "3521240/3521240 [==============================] - 7s 2us/sample - loss: 0.0300 - sparse_categorical_accuracy: 0.9908 - val_loss: 0.0382 - val_sparse_categorical_accuracy: 0.9890\n",
      "Epoch 00011: early stopping\n",
      "~~~~~ 4101_LC08_027038_20131103_1 70 % cloud cover\n",
      "Train on 2651187 samples, validate on 1136223 samples\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002592535D840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002592535D840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "2651187/2651187 [==============================] - 6s 2us/sample - loss: 0.0874 - sparse_categorical_accuracy: 0.9846 - val_loss: 0.0438 - val_sparse_categorical_accuracy: 0.9872\n",
      "Epoch 2/1000\n",
      "2651187/2651187 [==============================] - 5s 2us/sample - loss: 0.0380 - sparse_categorical_accuracy: 0.9892 - val_loss: 0.0383 - val_sparse_categorical_accuracy: 0.9881\n",
      "Epoch 3/1000\n",
      "2651187/2651187 [==============================] - 6s 2us/sample - loss: 0.0343 - sparse_categorical_accuracy: 0.9897 - val_loss: 0.0367 - val_sparse_categorical_accuracy: 0.9885\n",
      "Epoch 4/1000\n",
      "2651187/2651187 [==============================] - 6s 2us/sample - loss: 0.0324 - sparse_categorical_accuracy: 0.9901 - val_loss: 0.0352 - val_sparse_categorical_accuracy: 0.9894\n",
      "Epoch 5/1000\n",
      "2651187/2651187 [==============================] - 6s 2us/sample - loss: 0.0306 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.0337 - val_sparse_categorical_accuracy: 0.9899\n",
      "Epoch 6/1000\n",
      "2651187/2651187 [==============================] - 4s 2us/sample - loss: 0.0298 - sparse_categorical_accuracy: 0.9908 - val_loss: 0.0334 - val_sparse_categorical_accuracy: 0.9899\n",
      "Epoch 7/1000\n",
      "2651187/2651187 [==============================] - 4s 2us/sample - loss: 0.0291 - sparse_categorical_accuracy: 0.9911 - val_loss: 0.0333 - val_sparse_categorical_accuracy: 0.9902\n",
      "Epoch 8/1000\n",
      "2651187/2651187 [==============================] - 4s 2us/sample - loss: 0.0285 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.0337 - val_sparse_categorical_accuracy: 0.9902\n",
      "Epoch 9/1000\n",
      "2651187/2651187 [==============================] - 4s 1us/sample - loss: 0.0280 - sparse_categorical_accuracy: 0.9914 - val_loss: 0.0339 - val_sparse_categorical_accuracy: 0.9901\n",
      "Epoch 10/1000\n",
      "2651187/2651187 [==============================] - 4s 1us/sample - loss: 0.0277 - sparse_categorical_accuracy: 0.9914 - val_loss: 0.0338 - val_sparse_categorical_accuracy: 0.9899\n",
      "Epoch 11/1000\n",
      "2651187/2651187 [==============================] - 4s 1us/sample - loss: 0.0273 - sparse_categorical_accuracy: 0.9915 - val_loss: 0.0339 - val_sparse_categorical_accuracy: 0.9901\n",
      "Epoch 00011: early stopping\n",
      "~~~~~ 4101_LC08_027038_20131103_1 80 % cloud cover\n",
      "Train on 1772953 samples, validate on 759837 samples\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025928672B70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025928672B70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "1772953/1772953 [==============================] - 6s 3us/sample - loss: 0.1195 - sparse_categorical_accuracy: 0.9774 - val_loss: 0.0343 - val_sparse_categorical_accuracy: 0.9920\n",
      "Epoch 2/1000\n",
      "1772953/1772953 [==============================] - 4s 2us/sample - loss: 0.0382 - sparse_categorical_accuracy: 0.9894 - val_loss: 0.0281 - val_sparse_categorical_accuracy: 0.9922\n",
      "Epoch 3/1000\n",
      "1772953/1772953 [==============================] - 4s 2us/sample - loss: 0.0339 - sparse_categorical_accuracy: 0.9896 - val_loss: 0.0265 - val_sparse_categorical_accuracy: 0.9923\n",
      "Epoch 4/1000\n",
      "1772953/1772953 [==============================] - 4s 2us/sample - loss: 0.0319 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.0258 - val_sparse_categorical_accuracy: 0.9927\n",
      "Epoch 5/1000\n",
      "1772953/1772953 [==============================] - 4s 2us/sample - loss: 0.0308 - sparse_categorical_accuracy: 0.9902 - val_loss: 0.0249 - val_sparse_categorical_accuracy: 0.9928\n",
      "Epoch 6/1000\n",
      "1772953/1772953 [==============================] - 4s 2us/sample - loss: 0.0295 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.0241 - val_sparse_categorical_accuracy: 0.9930\n",
      "Epoch 7/1000\n",
      "1772953/1772953 [==============================] - 4s 2us/sample - loss: 0.0282 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.0240 - val_sparse_categorical_accuracy: 0.9931\n",
      "Epoch 8/1000\n",
      "1772953/1772953 [==============================] - 4s 2us/sample - loss: 0.0277 - sparse_categorical_accuracy: 0.9911 - val_loss: 0.0233 - val_sparse_categorical_accuracy: 0.9932\n",
      "Epoch 9/1000\n",
      "1772953/1772953 [==============================] - 4s 2us/sample - loss: 0.0271 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.0230 - val_sparse_categorical_accuracy: 0.9931\n",
      "Epoch 10/1000\n",
      "1772953/1772953 [==============================] - 4s 2us/sample - loss: 0.0264 - sparse_categorical_accuracy: 0.9915 - val_loss: 0.0229 - val_sparse_categorical_accuracy: 0.9933\n",
      "Epoch 11/1000\n",
      "1772953/1772953 [==============================] - 4s 2us/sample - loss: 0.0261 - sparse_categorical_accuracy: 0.9916 - val_loss: 0.0225 - val_sparse_categorical_accuracy: 0.9934\n",
      "Epoch 00011: early stopping\n",
      "~~~~~ 4101_LC08_027038_20131103_1 90 % cloud cover\n",
      "Train on 888854 samples, validate on 380937 samples\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025925698F28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000025925698F28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "888854/888854 [==============================] - 4s 4us/sample - loss: 0.3063 - sparse_categorical_accuracy: 0.8897 - val_loss: 0.0749 - val_sparse_categorical_accuracy: 0.9931\n",
      "Epoch 2/1000\n",
      "888854/888854 [==============================] - 2s 3us/sample - loss: 0.0606 - sparse_categorical_accuracy: 0.9913 - val_loss: 0.0366 - val_sparse_categorical_accuracy: 0.9944\n",
      "Epoch 3/1000\n",
      "888854/888854 [==============================] - 2s 2us/sample - loss: 0.0410 - sparse_categorical_accuracy: 0.9920 - val_loss: 0.0303 - val_sparse_categorical_accuracy: 0.9944\n",
      "Epoch 4/1000\n",
      "888854/888854 [==============================] - 1s 2us/sample - loss: 0.0352 - sparse_categorical_accuracy: 0.9922 - val_loss: 0.0278 - val_sparse_categorical_accuracy: 0.9946\n",
      "Epoch 5/1000\n",
      "888854/888854 [==============================] - 1s 2us/sample - loss: 0.0321 - sparse_categorical_accuracy: 0.9924 - val_loss: 0.0262 - val_sparse_categorical_accuracy: 0.9947\n",
      "Epoch 6/1000\n",
      "888854/888854 [==============================] - 1s 2us/sample - loss: 0.0305 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.0248 - val_sparse_categorical_accuracy: 0.9948\n",
      "Epoch 7/1000\n",
      "888854/888854 [==============================] - 1s 1us/sample - loss: 0.0288 - sparse_categorical_accuracy: 0.9927 - val_loss: 0.0242 - val_sparse_categorical_accuracy: 0.9948\n",
      "Epoch 8/1000\n",
      "888854/888854 [==============================] - 1s 2us/sample - loss: 0.0272 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.0237 - val_sparse_categorical_accuracy: 0.9948\n",
      "Epoch 9/1000\n",
      "888854/888854 [==============================] - 1s 2us/sample - loss: 0.0265 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.0237 - val_sparse_categorical_accuracy: 0.9947\n",
      "Epoch 10/1000\n",
      "888854/888854 [==============================] - 1s 2us/sample - loss: 0.0260 - sparse_categorical_accuracy: 0.9933 - val_loss: 0.0228 - val_sparse_categorical_accuracy: 0.9947\n",
      "Epoch 11/1000\n",
      "888854/888854 [==============================] - 2s 2us/sample - loss: 0.0254 - sparse_categorical_accuracy: 0.9933 - val_loss: 0.0230 - val_sparse_categorical_accuracy: 0.9949\n",
      "Epoch 00011: early stopping\n",
      "Metrics directory already exists\n"
     ]
    }
   ],
   "source": [
    "valMetricsList = []\n",
    "\n",
    "# Stack layers into a single tif, and generate cloud cover image\n",
    "for j, img in enumerate(img_list):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    accuracy = []\n",
    "    times = []\n",
    "    history = []\n",
    "    \n",
    "    tif_stacker(data_path, img, feat_list_new, overwrite=False)\n",
    "    cloud_generator(img, data_path, overwrite=False)\n",
    "    \n",
    "    for i, pctl in enumerate(pctls):\n",
    "        data_train, data_vector_train, data_ind_train = preprocessing(data_path, img, pctl, gaps=False)\n",
    "        training_data, validation_data = train_val(data_vector_train, holdout=holdout)\n",
    "        X_train, y_train = training_data[:,0:14], training_data[:,14]\n",
    "        X_val, y_val = validation_data[:,0:14], validation_data[:,14]\n",
    "        INPUT_DIMS = X_train.shape[1]\n",
    "                           \n",
    "        print('~~~~~', img, pctl,'% cloud cover')\n",
    "        \n",
    "        NN_MCD = get_NN_MCD()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        NN_MCD.fit(X_train, y_train,\n",
    "                   batch_size = batch_size,\n",
    "                   epochs= epochs,\n",
    "                   verbose = 1,\n",
    "                   validation_data = (X_val, y_val),\n",
    "                   callbacks=cb_list,\n",
    "                   use_multiprocessing = True)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        times.append(timer(start_time, end_time, False))\n",
    "\n",
    "        model_path = data_path / 'models' / 'cnn_vary_clouds' / img / '{0}'.format(img+'_clouds_'+str(pctl)+'.h5')\n",
    "        NN_MCD.save(model_path)\n",
    "    \n",
    "    metrics_path = data_path / 'metrics' / 'testing' / img \n",
    "    \n",
    "    try:\n",
    "        metrics_path.mkdir(parents=True)\n",
    "    except FileExistsError:\n",
    "        print('Metrics directory already exists')\n",
    "    \n",
    "    times = [float(i) for i in times]\n",
    "    times_df = pd.DataFrame(times, columns = ['times'])\n",
    "    times_df.to_csv(metrics_path / 'training_times.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4101_LC08_027038_20131103_1 10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'INPUT_DIMS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e08a0a1a01db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mNN_MCD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_NN_MCD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Get untrained model to add trained weights into\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'models'\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'cnn_vary_clouds'\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'{0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_clouds_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpctl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mNN_MCD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-69d37160e235>\u001b[0m in \u001b[0;36mget_NN_MCD\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mINPUT_DIMS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDROPOUT_RATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     model.add(tf.keras.layers.Dense(units=24,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'INPUT_DIMS' is not defined"
     ]
    }
   ],
   "source": [
    "# There is a problem loading keras models: https://github.com/keras-team/keras/issues/10417\n",
    "# But loading the weights into an identical compiled model works\n",
    "\n",
    "import pickle\n",
    "\n",
    "def predict_with_uncertainty(model, X):\n",
    "    preds = []\n",
    "    for i in range(MC_PASSES):\n",
    "        if i % 10 == 0 or i == MC_PASSES - 1:\n",
    "            print('Running MC '+str(i)+'/'+str(MC_PASSES))\n",
    "        preds.append(model.predict(X, batch_size=7000, use_multiprocessing=True))\n",
    "    preds = np.array(preds)\n",
    "    means = np.mean(preds, axis=0)\n",
    "    variances = np.var(preds, axis=0)\n",
    "    stds = np.std(preds, axis=0)\n",
    "    pred = np.argmax(means, axis=1)\n",
    "    pred = list(pred)\n",
    "#     return pred, preds, means, variances, stds\n",
    "    return pred, variances\n",
    "\n",
    "MC_PASSES = 100\n",
    "\n",
    "for j, img in enumerate(img_list):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    accuracy = []\n",
    "    times = []\n",
    "    predictions = []\n",
    "    gapMetricsList = []\n",
    "    variances = []\n",
    "    \n",
    "    for i, pctl in enumerate(pctls):\n",
    "        data_test, data_vector_test, data_ind_test = preprocessing(data_path, img, pctl, gaps=True)\n",
    "        X_test, y_test = data_vector_test[:,0:14], data_vector_test[:,14]\n",
    "        \n",
    "        print(img, pctl)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        NN_MCD = get_NN_MCD() # Get untrained model to add trained weights into\n",
    "        model_path = data_path / 'models' / 'cnn_vary_clouds' / img / '{0}'.format(img+'_clouds_'+str(pctl)+'.h5')\n",
    "        NN_MCD.load_weights(str(model_path))\n",
    "        preds, variances = predict_with_uncertainty(NN_MCD, X_test)\n",
    "        \n",
    "        times.append(timer(start_time, time.time(), False)) # Elapsed time for MC simulations\n",
    "        predictions.append(list(preds))\n",
    "        accuracy.append(sklearn.metrics.accuracy_score(y_test, preds))\n",
    "        precision.append(sklearn.metrics.precision_score(y_test, preds))\n",
    "        recall.append(sklearn.metrics.recall_score(y_test, preds))\n",
    "        f1.append(sklearn.metrics.f1_score(y_test, preds))\n",
    "    \n",
    "    metrics_path = data_path / 'metrics' / 'testing' / img \n",
    "    \n",
    "    try:\n",
    "        metrics_path.mkdir(parents=True)\n",
    "    except FileExistsError:\n",
    "        print('Metrics directory already exists')\n",
    "        \n",
    "    with open(str(metrics_path / 'predictions.pkl'), 'wb') as outfile:\n",
    "        pickle.dump(predictions, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    times = [float(i) for i in times] # Need to convert time objects to float, otherwise valMetrics will be non-numeric\n",
    "        \n",
    "    gapMetrics = pd.DataFrame(np.column_stack([pctls, accuracy, precision, recall, f1, times]),\n",
    "                          columns=['cloud_cover','accuracy','precision','recall','f1', 'time'])\n",
    "    \n",
    "    gapMetrics.to_csv(metrics_path / 'gapMetrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look into using GPU to improve speed\n",
    "# Also callbacks like EarlyStopping to halt training once performance stops improving"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
