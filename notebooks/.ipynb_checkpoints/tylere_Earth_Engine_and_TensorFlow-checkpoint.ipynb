{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Introduction to Earth Engine and TensorFlow in Cloud Datalab\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook walks you through a simple example of using Earth Engine and TensorFlow together in Cloud Datalab.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Specifically, we will train a neural network to recognize cloudy pixels in a Landsat scene. For this simple example we will use the output of the Fmask cloud detection algorithm as training data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Configure the Environment\\n\",\n",
    "    \"We begin by importing a number of useful libraries.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import ee\\n\",\n",
    "    \"from IPython import display\\n\",\n",
    "    \"import math\\n\",\n",
    "    \"from matplotlib import pyplot\\n\",\n",
    "    \"import numpy\\n\",\n",
    "    \"from osgeo import gdal\\n\",\n",
    "    \"import tempfile\\n\",\n",
    "    \"import tensorflow as tf\\n\",\n",
    "    \"import urllib\\n\",\n",
    "    \"import zipfile\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Initialize the Earth Engine client. This assumes that you have already configured Earth Engine credentials in this Datalab instance. If not, see the \\\"`Earth Engine Datalab Initialization.ipynb`\\\" notebook.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"ee.Initialize()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Inspect the Input Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Load a Landsat image with corresponding Fmask label data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"input_image = ee.Image('LANDSAT/LT5_L1T_TOA_FMASK/LT50100551998003CPE00')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Let's define a helper function to make it easier to print thumbnails of Earth Engine images. (We'll be adding a library with utility functions like this one to the Earth Engine Python SDK, but for now we can do it by hand.)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def print_image(image):\\n\",\n",
    "    \"  display.display(display.Image(ee.data.getThumbnail({\\n\",\n",
    "    \"      'image': image.serialize(),\\n\",\n",
    "    \"      'dimensions': '360',\\n\",\n",
    "    \"  })))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Now we can use our helper function to quickly visualize the image and label data. The Fmask values are:\\n\",\n",
    "    \"\\n\",\n",
    "    \"0 | 1 | 2 | 3 | 4\\n\",\n",
    "    \":---:|:---:|:---:|:---:|:---:\\n\",\n",
    "    \"Clear | Water | Shadow | Snow | Cloud\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print_image(input_image.visualize(\\n\",\n",
    "    \"    bands=['B3', 'B2', 'B1'],\\n\",\n",
    "    \"    min=0,\\n\",\n",
    "    \"    max=0.3,\\n\",\n",
    "    \"))\\n\",\n",
    "    \"print_image(input_image.visualize(\\n\",\n",
    "    \"    bands=['fmask'],\\n\",\n",
    "    \"    min=0,\\n\",\n",
    "    \"    max=4,\\n\",\n",
    "    \"    palette=['808080', '0000C0', '404040', '00FFFF', 'FFFFFF'],\\n\",\n",
    "    \"))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Fetch the Input Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"First we define some helper functions to download raw data from Earth Engine as `numpy` arrays.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We use the `getDownloadId()` function, which only works for modestly sized datasets. For larger datasets, a better approach would be to initiate a batch Export from Earth Engine to Cloud Storage, which you could easily manage right here in Datalab too.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def download_tif(image, scale):\\n\",\n",
    "    \"  url = ee.data.makeDownloadUrl(ee.data.getDownloadId({\\n\",\n",
    "    \"        'image': image.serialize(),\\n\",\n",
    "    \"        'scale': '%d' % scale,\\n\",\n",
    "    \"        'filePerBand': 'false',\\n\",\n",
    "    \"        'name': 'data',\\n\",\n",
    "    \"  }))\\n\",\n",
    "    \"  local_zip, headers = urllib.urlretrieve(url)\\n\",\n",
    "    \"  with zipfile.ZipFile(local_zip) as local_zipfile:\\n\",\n",
    "    \"    return local_zipfile.extract('data.tif', tempfile.mkdtemp())\\n\",\n",
    "    \"\\n\",\n",
    "    \"def load_image(image, scale):\\n\",\n",
    "    \"  local_tif_filename = download_tif(image, scale)\\n\",\n",
    "    \"  dataset = gdal.Open(local_tif_filename, gdal.GA_ReadOnly)\\n\",\n",
    "    \"  bands = [dataset.GetRasterBand(i + 1).ReadAsArray() for i in range(dataset.RasterCount)]\\n\",\n",
    "    \"  return numpy.stack(bands, 2)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Now we can use that function to load the data from Earth Engine, including a valid data band, as a `numpy` array. This may take a few seconds. We also convert the Fmask band  to a binary cloud label (i.e. `fmask`=4).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"mask = input_image.mask().reduce('min')\\n\",\n",
    "    \"data = load_image(input_image.addBands(mask), scale=240)\\n\",\n",
    "    \"data[:,:,7] = numpy.equal(data[:,:,7], 4)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Display the local data. This time, for variety, we display it as an NRG false-color image. We can use `pyplot` to display local `numpy` arrays.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pyplot.imshow(numpy.clip(data[:,:,[3,2,1]] * 3, 0, 1))\\n\",\n",
    "    \"pyplot.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Preprocess the Input Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Select the valid pixels and hold out a fraction for use as validation data.  Compute per-band means and standard deviations of the training data for normalization.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"HOLDOUT_FRACTION = 0.1\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Reshape into a single vector of pixels.\\n\",\n",
    "    \"data_vector = data.reshape([data.shape[0] * data.shape[1], data.shape[2]])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Select only the valid data and shuffle it.\\n\",\n",
    "    \"valid_data = data_vector[numpy.equal(data_vector[:,8], 1)]\\n\",\n",
    "    \"numpy.random.shuffle(valid_data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Hold out a fraction of the labeled data for validation.\\n\",\n",
    "    \"training_size = int(valid_data.shape[0] * (1 - HOLDOUT_FRACTION))\\n\",\n",
    "    \"training_data = valid_data[0:training_size,:]\\n\",\n",
    "    \"validation_data = valid_data[training_size:-1,:]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Compute per-band means and standard deviations of the input bands.\\n\",\n",
    "    \"data_mean = training_data[:,0:7].mean(0)\\n\",\n",
    "    \"data_std = training_data[:,0:7].std(0)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"valid_data.shape\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Build the TensorFlow Model\\n\",\n",
    "    \"\\n\",\n",
    "    \"We start with a helper function to build a simple TensorFlow neural network layer.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def make_nn_layer(input, output_size):\\n\",\n",
    "    \"  input_size = input.get_shape().as_list()[1]\\n\",\n",
    "    \"  weights = tf.Variable(tf.truncated_normal(\\n\",\n",
    "    \"      [input_size, output_size],\\n\",\n",
    "    \"      stddev=1.0 / math.sqrt(float(input_size))))\\n\",\n",
    "    \"  biases = tf.Variable(tf.zeros([output_size]))\\n\",\n",
    "    \"  return tf.matmul(input, weights) + biases\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Here we define our TensorFlow model, a neural network with two hidden layers with tanh() nonlinearities. The main network has two outputs, continuous-valued “logits” representing non-cloud and cloud, respectively. The binary output is intepreted as the argmax of these outputs.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We define a training step, which uses Kingma and Ba's Adam algorithm to minimize the cross-entropy between the logits and the training data. Finally, we define a simple overall percentage accuracy measure.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"NUM_INPUT_BANDS = 7\\n\",\n",
    "    \"NUM_HIDDEN_1 = 20\\n\",\n",
    "    \"NUM_HIDDEN_2 = 20\\n\",\n",
    "    \"NUM_CLASSES = 2\\n\",\n",
    "    \"\\n\",\n",
    "    \"input = tf.placeholder(tf.float32, shape=[None, NUM_INPUT_BANDS])\\n\",\n",
    "    \"labels = tf.placeholder(tf.float32, shape=[None])\\n\",\n",
    "    \"\\n\",\n",
    "    \"normalized = (input - data_mean) / data_std\\n\",\n",
    "    \"hidden1 = tf.nn.tanh(make_nn_layer(normalized, NUM_HIDDEN_1))\\n\",\n",
    "    \"hidden2 = tf.nn.tanh(make_nn_layer(hidden1, NUM_HIDDEN_2))\\n\",\n",
    "    \"logits = make_nn_layer(hidden2, NUM_CLASSES)\\n\",\n",
    "    \"outputs = tf.argmax(logits, 1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"int_labels = tf.to_int64(labels)\\n\",\n",
    "    \"cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, int_labels, name='xentropy')\\n\",\n",
    "    \"train_step = tf.train.AdamOptimizer().minimize(cross_entropy)\\n\",\n",
    "    \"\\n\",\n",
    "    \"correct_prediction = tf.equal(outputs, int_labels)\\n\",\n",
    "    \"accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Train the Neural Network\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now train the neural network, using batches of training data drawn randomly from the training data pool. We periodically compute the accuracy against the validation data. When we're done training, we apply the model to the complete input data set.\\n\",\n",
    "    \"\\n\",\n",
    "    \"This simple notebook performs all TensorFlow operations locally. However, for larger analyses you could bring up a cluster of TensorFlow workers to parallelize the computation, all controlled from within Datalab.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"BATCH_SIZE = 1000\\n\",\n",
    "    \"NUM_BATCHES = 1000\\n\",\n",
    "    \"\\n\",\n",
    "    \"with tf.Session() as sess:\\n\",\n",
    "    \"  sess.run(tf.initialize_all_variables())\\n\",\n",
    "    \"\\n\",\n",
    "    \"  validation_dict = {\\n\",\n",
    "    \"    input: validation_data[:,0:7],\\n\",\n",
    "    \"    labels: validation_data[:,7],\\n\",\n",
    "    \"  }\\n\",\n",
    "    \"\\n\",\n",
    "    \"  for i in range(NUM_BATCHES):\\n\",\n",
    "    \"    batch = training_data[numpy.random.choice(training_size, BATCH_SIZE, False),:]\\n\",\n",
    "    \"    train_step.run({input: batch[:,0:7], labels: batch[:,7]})\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if i % 100 == 0 or i == NUM_BATCHES - 1:\\n\",\n",
    "    \"      print('Accuracy %.2f%% at step %d' % (accuracy.eval(validation_dict) * 100, i))\\n\",\n",
    "    \"\\n\",\n",
    "    \"  output_data = outputs.eval({input: data_vector[:,0:7]})\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Inspect the Results\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here we dislay the results. The red band corresponds to the TensorFlow output and the blue band corresponds to the labeled training data, so pixels that are red and blue correspond to disagreements between the model and the training data. (There aren't many: look carefully around the fringes of the clouds.)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"output_image = output_data.reshape([data.shape[0], data.shape[1]])\\n\",\n",
    "    \"red = numpy.where(data[:,:,8], output_image, 0.5)\\n\",\n",
    "    \"blue = numpy.where(data[:,:,8], data[:,:,7], 0.5)\\n\",\n",
    "    \"green = numpy.minimum(red, blue)\\n\",\n",
    "    \"\\n\",\n",
    "    \"comparison_image = numpy.dstack((red, green, blue))\\n\",\n",
    "    \"pyplot.figure(figsize = (12,12))\\n\",\n",
    "    \"pyplot.imshow(comparison_image)\\n\",\n",
    "    \"pyplot.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"We can zoom in on a particular region over on the right side of the image to see some of the disagreements. Red pixels represent comission errors and blue pixels represent omission errors relative to the labeled input data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": false\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pyplot.figure(figsize = (12,12))\\n\",\n",
    "    \"pyplot.imshow(comparison_image[300:500,600:,:], interpolation='nearest')\\n\",\n",
    "    \"pyplot.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 2\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python2\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 2\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython2\",\n",
    "   \"version\": \"2.7.12\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 0\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
