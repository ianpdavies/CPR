{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing CNN and RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than training RF on 30k sample points, this time I'm going to train it on the entire image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "import ee\n",
    "import os\n",
    "import rasterio\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "path_clear = 'C:/Users/ipdavies/CPR/data/images/clear_4337_LC08_026038_20160325'\n",
    "path_clouds = 'C:/Users/ipdavies/CPR/data/images/clouds_4337_LC08_026038_20160325'\n",
    "model_path = 'C:/Users/ipdavies/CPR/data/models/'\n",
    "\n",
    "# Misc functions\n",
    "import time\n",
    "def timer(start,end):\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    return str(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest with oversampling of flooded pixels\n",
    "### Load images, stack and convert to np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\rasterio\\__init__.py:160: FutureWarning: GDAL-style transforms are deprecated and will not be supported in Rasterio 1.0.\n",
      "  transform = guard_transform(transform)\n"
     ]
    }
   ],
   "source": [
    "# Stack exported tifs from GEE into one multiband tif\n",
    "\n",
    "def listdir_fullpath(d):\n",
    "    return [os.path.join(d, f) for f in os.listdir(d)]\n",
    "\n",
    "file_list = []\n",
    "for file in listdir_fullpath(path_clear):\n",
    "    if file.endswith('.tif'):\n",
    "        file_list.append(file)\n",
    "\n",
    "feat_list_files = list(map(lambda x: x.split('.')[-2], file_list)) # list of features in file order\n",
    "\n",
    "\n",
    "#=========== Want to rearrange the order of files so that target feature is last\n",
    "\n",
    "# Create list of features with target feature (flooded) last\n",
    "feat_list_new = ['aspect','curve', 'developed', 'distExtent', 'elevation', 'forest',\n",
    " 'GSW_maxExtent', 'hand', 'other_landcover', 'planted', 'slope', 'spi', 'twi', 'wetlands', 'flooded']\n",
    "\n",
    "# Create 1 row df of file names where each col is a feature name, in the order files are stored locally\n",
    "file_arr = pd.DataFrame(data=[file_list], columns=feat_list_files)\n",
    "\n",
    "# Then index the file list by the ordered list of feature names used in training\n",
    "file_arr = file_arr.loc[:, feat_list_new]\n",
    "\n",
    "# The take this re-ordered row as a list - the new file_list\n",
    "file_list = list(file_arr.iloc[0,:])\n",
    "    \n",
    "# Read metadata of first file. This needs to be a band in float32 dtype, because it sets the metadata for the entire stack\n",
    "# and we are converting the other bands to float64\n",
    "with rasterio.open(file_list[1]) as src0:\n",
    "    meta = src0.meta\n",
    "    meta['dtype'] = 'float32'\n",
    "#         print(meta)\n",
    "\n",
    "# Update meta to reflect the number of layers\n",
    "meta.update(count = len(file_list))\n",
    "\n",
    "# Read each layer, convert to float, and write it to stack\n",
    "# There's also a gdal way to do this, but unsure how to convert to float: https://gis.stackexchange.com/questions/223910/using-rasterio-or-gdal-to-stack-multiple-bands-without-using-subprocess-commands\n",
    "\n",
    "# Make new directory for stacked tif if it doesn't already exist\n",
    "try:\n",
    "    os.mkdir(path_clear+'/stack')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# Remove stack file if already exists\n",
    "try:\n",
    "    os.remove(path_clear + '/stack/stack.tif')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with rasterio.open(path_clear + '/stack/stack.tif', 'w', **meta) as dst:\n",
    "    for id, layer in enumerate(file_list, start=0):\n",
    "        with rasterio.open(layer) as src1:\n",
    "            dst.write_band(id+1, src1.read(1).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14000674"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the array\n",
    "\n",
    "# Get local image\n",
    "with rasterio.open(path_clear + '/stack/stack.tif', 'r') as ds:\n",
    "    data = ds.read()\n",
    "    data = data.transpose((1, -1, 0)) # Not sure why the rasterio.read output is originally (D, W, H)\n",
    "\n",
    "# Need to remove NaNs because any arithmetic operation involving an NaN will result in NaN\n",
    "\n",
    "# Convert -999999 to None\n",
    "data[data == -999999] = np.nan\n",
    "\n",
    "# Get indices of non-nan values. These are the indices of the original image array\n",
    "# data_ind = np.where(data[:,:,1] != None)\n",
    "data_ind = np.where(~np.isnan(data[:,:,1]))\n",
    "row, col = zip(np.where(~np.isnan(data[:,:,1]))) # image row and col of values\n",
    "len(*row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle, split into train/test, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Can't train on the entire image, so let's take a random sample\n",
    "\n",
    "# HOLDOUT_FRACTION = 0.1\n",
    "\n",
    "# # Reshape into a single vector of pixels.\n",
    "# data_vector = data.reshape([data.shape[0] * data.shape[1], data.shape[2]])\n",
    "\n",
    "# # Remove NaNs\n",
    "# data_vector = data_vector[~np.isnan(data_vector).any(axis=1)]\n",
    "# data_vector.shape\n",
    "\n",
    "# # Shuffle data\n",
    "# np.random.shuffle(data_vector)\n",
    "\n",
    "# # Compute per-band means and standard deviations of the input bands.\n",
    "# data_mean = training_data[:,0:14].mean(0)\n",
    "# data_std = training_data[:,0:14].std(0)\n",
    "\n",
    "# # Hold out a fraction of the labeled data for validation.\n",
    "# training_size = int(data_vector.shape[0] * (1 - HOLDOUT_FRACTION))\n",
    "# training_data = data_vector[0:training_size,:]\n",
    "# validation_data = data_vector[training_size:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's create another sample, this time stratified flood/non-flood. We oversample flooded pixels\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# # Reshape into a single vector of pixels.\n",
    "# data_vector = data.reshape([data.shape[0] * data.shape[1], data.shape[2]])\n",
    "\n",
    "# # Remove NaNs\n",
    "# data_vector = data_vector[~np.isnan(data_vector).any(axis=1)]\n",
    "# data_vector.shape\n",
    "\n",
    "# # Compute per-band means and standard deviations of the input bands.\n",
    "# data_mean = data_vector[:,0:14].mean(0)\n",
    "# data_std = data_vector[:,0:14].std(0)\n",
    "\n",
    "# # Normalize data\n",
    "# X = (data_vector[:,0:14] - data_mean) / data_std\n",
    "# y = data_vector[:,14]\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, random_state=12)\n",
    "# sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "# X_train_res, y_train_res = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000674, 15)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sm.fit_sample is taking way too long to run, so let's try just running it on a sample.\n",
    "# This time with ALL flooded pixels and a subset of non-flooded pixels\n",
    "\n",
    "# Reshape into a single vector of pixels.\n",
    "data_vector = data.reshape([data.shape[0] * data.shape[1], data.shape[2]])\n",
    "\n",
    "# Remove NaNs\n",
    "data_vector = data_vector[~np.isnan(data_vector).any(axis=1)]\n",
    "data_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_per = 0.1 # Size of sample as a %\n",
    "\n",
    "training_size = int(data_vector.shape[0] * (sample_per)) \n",
    "training_data = data_vector[0:training_size,:]\n",
    "validation_data = data_vector[training_size:-1,:]\n",
    "\n",
    "flooded = data_vector[data_vector[:,14]==1] # Select only flooded pixels\n",
    "np.random.shuffle(flooded) # Shuffle\n",
    "sample = data_vector[data_vector[:,14]==0] # Select only non-flooded pixels\n",
    "np.random.shuffle(sample) # Shuffle\n",
    "\n",
    "sample_size = int(sample.shape[0] * (sample_per)) # Get sample size in number of rows\n",
    "sample = sample[0:sample_size,:] # Sample non-flooded pixels\n",
    "training_data = np.concatenate((flooded, sample), axis=0) # Combine flooded and non-flooded\n",
    "training_data.shape\n",
    "\n",
    "# Compute per-band means and standard deviations of the input bands.\n",
    "data_mean = training_data[:,0:14].mean(0)\n",
    "data_std = training_data[:,0:14].std(0)\n",
    "\n",
    "# Normalize data\n",
    "X = (training_data[:,0:14] - data_mean) / data_std\n",
    "y = training_data[:,14]\n",
    "\n",
    "X_train_res, X_val_res, y_train_res, y_val_res = train_test_split(X, y, test_size = 0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No oversampling\n",
    "\n",
    "sample_per = 0.1 # Size of sample as a %\n",
    "\n",
    "sample = data_vector\n",
    "\n",
    "np.random.shuffle(sample) # Shuffle dataset\n",
    "\n",
    "# Sample a percentage of pixels\n",
    "sample_size = int(sample.shape[0] * sample_per)\n",
    "sample = sample[0:sample_size,:]\n",
    "\n",
    "# Compute per-band means and standard deviations of the input bands.\n",
    "data_mean = sample[:,0:14].mean(0)\n",
    "data_std = sample[:,0:14].std(0)\n",
    "\n",
    "# Normalize data\n",
    "X = (data_vector[:,0:14] - data_mean) / data_std\n",
    "y = data_vector[:,14]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF training time: 00:02:58.21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_name = 'RF_oversamp'\n",
    "\n",
    "# Hyperparameters optimized for the 30k sample (using a random search)\n",
    "# best_n_estimators = 800\n",
    "best_n_estimators = 15\n",
    "best_min_samples_split = 10\n",
    "best_min_samples_leaf = 2\n",
    "best_max_features = 'sqrt'\n",
    "best_max_depth = 20\n",
    "best_bootstrap = False\n",
    "\n",
    "# Instantiate model with best parameters from random search\n",
    "clf = RandomForestClassifier(n_estimators = best_n_estimators, \n",
    "#                              max_depth = best_max_depth,\n",
    "#                              max_features = best_max_features,\n",
    "#                              min_samples_split = best_min_samples_split,\n",
    "#                              min_samples_leaf = best_min_samples_leaf,\n",
    "                             bootstrap = best_bootstrap,\n",
    "                             random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the RF model to data\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "print('RF training time: ' + timer(start_time, time.time()))\n",
    "\n",
    "# Save model\n",
    "pickle.dump(clf, open(model_path+model_name+'.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No oversampling RF\n",
    "\n",
    "model_name = 'RF_NoOversamp'\n",
    "\n",
    "# Hyperparameters optimized for the 30k sample (using a random search)\n",
    "# best_n_estimators = 800\n",
    "best_n_estimators = 15\n",
    "best_min_samples_split = 10\n",
    "best_min_samples_leaf = 2\n",
    "best_max_features = 'sqrt'\n",
    "best_max_depth = 20\n",
    "best_bootstrap = False\n",
    "\n",
    "# Instantiate model with best parameters from random search\n",
    "clf = RandomForestClassifier(n_estimators = best_n_estimators, \n",
    "#                              max_depth = best_max_depth,\n",
    "#                              max_features = best_max_features,\n",
    "#                              min_samples_split = best_min_samples_split,\n",
    "#                              min_samples_leaf = best_min_samples_leaf,\n",
    "                             bootstrap = best_bootstrap,\n",
    "                             random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the RF model to data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('RF training time: ' + timer(start_time, time.time()))\n",
    "\n",
    "# Save model\n",
    "pickle.dump(clf, open(model_path+model_name+'.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics of Random Forest models on train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:253: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.19.1 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:253: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.19.1 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_oversamp\n",
      "Classification report for classifier RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.86      0.92   2688332\n",
      "         1.0       0.22      0.94      0.36    111803\n",
      "\n",
      "   micro avg       0.87      0.87      0.87   2800135\n",
      "   macro avg       0.61      0.90      0.64   2800135\n",
      "weighted avg       0.97      0.87      0.90   2800135\n",
      "\n",
      "\n",
      "                  flooded notFlooded \n",
      "       flooded       82.8       13.2 \n",
      "    notFlooded        0.2        3.8 \n",
      "RF_NoOversamp\n",
      "Classification report for classifier RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.86      0.92   2688332\n",
      "         1.0       0.22      0.94      0.36    111803\n",
      "\n",
      "   micro avg       0.87      0.87      0.87   2800135\n",
      "   macro avg       0.61      0.90      0.64   2800135\n",
      "weighted avg       0.97      0.87      0.90   2800135\n",
      "\n",
      "\n",
      "                  flooded notFlooded \n",
      "       flooded       82.8       13.2 \n",
      "    notFlooded        0.2        3.8 \n",
      "RF\n",
      "Classification report for classifier RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=10,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=800, n_jobs=1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.06      0.12   2688332\n",
      "         1.0       0.04      0.99      0.08    111803\n",
      "\n",
      "   micro avg       0.10      0.10      0.10   2800135\n",
      "   macro avg       0.52      0.53      0.10   2800135\n",
      "weighted avg       0.96      0.10      0.12   2800135\n",
      "\n",
      "\n",
      "                  flooded notFlooded \n",
      "       flooded        6.2       89.8 \n",
      "    notFlooded        0.0        4.0 \n"
     ]
    }
   ],
   "source": [
    "# Import models\n",
    "clf_rf_oversamp = pickle.load(open(model_path+'RF_oversamp'+'.sav', 'rb'))\n",
    "clf_rf_NoOversamp = pickle.load(open(model_path+'RF_NoOversamp'+'.sav', 'rb'))\n",
    "clf_rf = pickle.load(open(model_path+'RF1'+'.sav', 'rb'))\n",
    "\n",
    "\n",
    "models = {'RF_oversamp':clf_rf_oversamp,\n",
    "          'RF_NoOversamp':clf_rf_NoOversamp,\n",
    "         'RF':clf_rf}\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "# Confusion matrix\n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(name)\n",
    "    print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (model, metrics.classification_report(y_val, y_pred)))\n",
    "    print_cm(metrics.confusion_matrix(y_val, y_pred)/len(y_pred)*100, labels=['flooded','notFlooded'])\n",
    "# Top row = predicted, left col = true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on gap filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\rasterio\\__init__.py:160: FutureWarning: GDAL-style transforms are deprecated and will not be supported in Rasterio 1.0.\n",
      "  transform = guard_transform(transform)\n"
     ]
    }
   ],
   "source": [
    "# Stack exported tifs from GEE into one multiband tif\n",
    "import rasterio\n",
    "import os\n",
    "\n",
    "path = '../data/images/clouds_4337_LC08_026038_20160325'\n",
    "\n",
    "def listdir_fullpath(d):\n",
    "    return [os.path.join(d, f) for f in os.listdir(d)]\n",
    "\n",
    "file_list = []\n",
    "for file in listdir_fullpath(path):\n",
    "    if file.endswith('.tif'):\n",
    "        file_list.append(file)\n",
    "\n",
    "feat_list_files = list(map(lambda x: x.split('.')[-2], file_list)) # list of features in file order\n",
    "\n",
    "#=========== Want to rearrange the order of files so that target feature is last\n",
    "\n",
    "# Create list of features with target feature (flooded) last\n",
    "feat_list_new = ['aspect','curve', 'developed', 'distExtent', 'elevation', 'forest',\n",
    " 'GSW_maxExtent', 'hand', 'other_landcover', 'planted', 'slope', 'spi', 'twi', 'wetlands', 'flooded']\n",
    "\n",
    "# Create 1 row df of file names where each col is a feature name, in the order files are stored locally\n",
    "file_arr = pd.DataFrame(data=[file_list], columns=feat_list_files)\n",
    "\n",
    "# Then index the file list by the ordered list of feature names used in training\n",
    "file_arr = file_arr.loc[:, feat_list_new]\n",
    "\n",
    "# The take this re-ordered row as a list - the new file_list\n",
    "file_list = list(file_arr.iloc[0,:])\n",
    "    \n",
    "# Read metadata of first file. This needs to be a band in float32 dtype, because it sets the metadata for the entire stack\n",
    "# and we are converting the other bands to float64\n",
    "with rasterio.open(file_list[1]) as src0:\n",
    "    meta = src0.meta\n",
    "    meta['dtype'] = 'float32'\n",
    "#         print(meta)\n",
    "\n",
    "# Update meta to reflect the number of layers\n",
    "meta.update(count = len(file_list))\n",
    "\n",
    "# Read each layer, convert to float, and write it to stack\n",
    "# There's also a gdal way to do this, but unsure how to convert to float: https://gis.stackexchange.com/questions/223910/using-rasterio-or-gdal-to-stack-multiple-bands-without-using-subprocess-commands\n",
    "\n",
    "# Make new directory for stacked tif if it doesn't already exist\n",
    "try:\n",
    "    os.mkdir(path+'/stack')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# Remove stack file if already exists\n",
    "try:\n",
    "    os.remove(path + '/stack/stack.tif')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with rasterio.open(path + '/stack/stack.tif', 'w', **meta) as dst:\n",
    "    for id, layer in enumerate(file_list, start=0):\n",
    "        with rasterio.open(layer) as src1:\n",
    "            dst.write_band(id+1, src1.read(1).astype('float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get local image\n",
    "with rasterio.open(path + '/stack/stack.tif', 'r') as ds:\n",
    "    data = ds.read()\n",
    "    data = data.transpose((1, -1, 0)) # Not sure why the rasterio.read output is originally (D, W, H)\n",
    "\n",
    "# Need to remove NaNs because any arithmetic operation involving an NaN will result in NaN\n",
    "\n",
    "# Convert -999999 to None\n",
    "data[data == -999999] = np.nan\n",
    "\n",
    "# Get indices of non-nan values. These are the indices of the original image array\n",
    "# data_ind = np.where(data[:,:,1] != None)\n",
    "data_ind = np.where(~np.isnan(data[:,:,1]))\n",
    "row, col = zip(np.where(~np.isnan(data[:,:,1]))) # image row and col of values\n",
    "len(*row)\n",
    "\n",
    "# Reshape into an array of pixels.\n",
    "data_vector = data.reshape([data.shape[0] * data.shape[1], data.shape[2]])\n",
    "\n",
    "# Remove NaNs\n",
    "data_vector = data_vector[~np.isnan(data_vector).any(axis=1)]\n",
    "data_vector.shape\n",
    "\n",
    "X = data_vector[:,0:14]\n",
    "y = data_vector[:,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:253: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.19.1 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\ipdavies\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:253: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.19.1 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_oversamp\n",
      "Classification report for classifier RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      1.00      0.94    970486\n",
      "         1.0       0.69      0.00      0.00    124961\n",
      "\n",
      "   micro avg       0.89      0.89      0.89   1095447\n",
      "   macro avg       0.79      0.50      0.47   1095447\n",
      "weighted avg       0.86      0.89      0.83   1095447\n",
      "\n",
      "\n",
      "                  flooded notFlooded \n",
      "       flooded       88.6        0.0 \n",
      "    notFlooded       11.4        0.0 \n",
      "RF_NoOversamp\n",
      "Classification report for classifier RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      1.00      0.94    970486\n",
      "         1.0       0.69      0.00      0.00    124961\n",
      "\n",
      "   micro avg       0.89      0.89      0.89   1095447\n",
      "   macro avg       0.79      0.50      0.47   1095447\n",
      "weighted avg       0.86      0.89      0.83   1095447\n",
      "\n",
      "\n",
      "                  flooded notFlooded \n",
      "       flooded       88.6        0.0 \n",
      "    notFlooded       11.4        0.0 \n",
      "RF\n",
      "Classification report for classifier RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=10,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=800, n_jobs=1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.97      0.93    970486\n",
      "         1.0       0.01      0.00      0.00    124961\n",
      "\n",
      "   micro avg       0.86      0.86      0.86   1095447\n",
      "   macro avg       0.45      0.49      0.46   1095447\n",
      "weighted avg       0.78      0.86      0.82   1095447\n",
      "\n",
      "\n",
      "                  flooded notFlooded \n",
      "       flooded       86.3        2.3 \n",
      "    notFlooded       11.4        0.0 \n"
     ]
    }
   ],
   "source": [
    "# Predict using models\n",
    "\n",
    "# Import models\n",
    "clf_rf_oversamp = pickle.load(open(model_path+'RF_oversamp'+'.sav', 'rb'))\n",
    "clf_rf_NoOversamp = pickle.load(open(model_path+'RF_NoOversamp'+'.sav', 'rb'))\n",
    "clf_rf = pickle.load(open(model_path+'RF1'+'.sav', 'rb'))\n",
    "\n",
    "\n",
    "models = {'RF_oversamp':clf_rf_oversamp,\n",
    "          'RF_NoOversamp':clf_rf_NoOversamp,\n",
    "         'RF':clf_rf}\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "# Confusion matrix\n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()\n",
    "        \n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X)\n",
    "    print(name)\n",
    "    print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (model, metrics.classification_report(y, y_pred)))\n",
    "    print_cm(metrics.confusion_matrix(y, y_pred)/len(y_pred)*100, labels=['flooded','notFlooded'])\n",
    "# Top row = predicted, left col = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "# Reshape predicted values back into image band\n",
    "with rasterio.open(path + '/stack/stack.tif', 'r') as ds:\n",
    "        shape = ds.read(2).shape # Shape of full original image\n",
    "        arr_empty = np.zeros(shape) # Create empty array with this shape\n",
    "\n",
    "output_image = arr_empty\n",
    "rows, cols = zip(data_ind)\n",
    "output_image[rows, cols] = output_data\n",
    "\n",
    "plt.figure(figsize=(20,30))\n",
    "columns = 2\n",
    "images = [output_image, data[:,:,14]]\n",
    "titles = ['Predicted Flooding','Actual Flooding']\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
    "    plt.title(titles[i], fontdict = {'fontsize' : 18})\n",
    "    plt.imshow(image)\n",
    "    plt.colorbar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
